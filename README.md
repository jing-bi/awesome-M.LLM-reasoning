# Awesome MLLM Reasoning [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

### ðŸ”¥ðŸ”¥ðŸ”¥ [MultiModal Language Models Reasoning: A Survey]




## ðŸ“° Table of Contents
- [Background](#-background)
- [Paper List](#-paper-list)


## ðŸŽ‰ Background
With the rapid advancement of Multimodal Large Language Models (MLLMs), their reasoning capabilities have become a focal point of research. Understanding how these models process and integrate information across different modalities is essential for improving interpretability and reliability. These developments have led to an increasing body of work analyzing the fundamental reasoning capabilities and Aha Moment within LMLMs, which we refer to as Multimodal Reasoning Paradigms.


## ðŸ“š Paper List
| Paper | Title |
| --- | --- |
| [https://www.arxiv.org/abs/2502.02339](https://www.arxiv.org/abs/2502.02339) | Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking |
| [https://www.arxiv.org/abs/2502.00711](https://www.arxiv.org/abs/2502.00711) | VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework |
| [https://www.arxiv.org/abs/2502.00382](https://www.arxiv.org/abs/2502.00382) | Masked Generative Nested Transformers with Decode Time Scaling |
| [https://www.arxiv.org/abs/2501.19393](https://www.arxiv.org/abs/2501.19393) | s1: Simple test-time scaling |
| [https://www.arxiv.org/abs/2501.18880](https://www.arxiv.org/abs/2501.18880) | RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception |
| [https://www.arxiv.org/abs/2501.18585](https://www.arxiv.org/abs/2501.18585) | Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs |
| [https://www.arxiv.org/abs/2501.13926](https://www.arxiv.org/abs/2501.13926) | Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step |
| [https://www.arxiv.org/abs/2501.13536](https://www.arxiv.org/abs/2501.13536) | ReasVQA: Advancing VideoQA with Imperfect Reasoning Process |
| [https://www.arxiv.org/abs/2501.12368](https://www.arxiv.org/abs/2501.12368) | InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model |
| [https://www.arxiv.org/abs/2501.12273](https://www.arxiv.org/abs/2501.12273) | Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement |
| [https://www.arxiv.org/abs/2501.10799](https://www.arxiv.org/abs/2501.10799) | Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback |
| [https://www.arxiv.org/abs/2501.10674](https://www.arxiv.org/abs/2501.10674) | Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No! |
| [https://www.arxiv.org/abs/2501.09891](https://www.arxiv.org/abs/2501.09891) | Evolving Deeper LLM Thinking |
| [https://www.arxiv.org/abs/2501.09751](https://www.arxiv.org/abs/2501.09751) | OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking |
| [https://www.arxiv.org/abs/2501.09686](https://www.arxiv.org/abs/2501.09686) | Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models |
| [https://www.arxiv.org/abs/2501.08617](https://www.arxiv.org/abs/2501.08617) | RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation |
| [https://www.arxiv.org/abs/2501.07542](https://www.arxiv.org/abs/2501.07542) | Imagine while Reasoning in Space: Multimodal Visualization-of-Thought |
| [https://www.arxiv.org/abs/2501.07214](https://www.arxiv.org/abs/2501.07214) | TimeLogic: A Temporal Logic Benchmark for Video QA |
| [https://www.arxiv.org/abs/2501.06430](https://www.arxiv.org/abs/2501.06430) | Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs |
| [https://www.arxiv.org/abs/2501.05727](https://www.arxiv.org/abs/2501.05727) | Enabling Scalable Oversight via Self-Evolving Critic |
| [https://www.arxiv.org/abs/2501.05444](https://www.arxiv.org/abs/2501.05444) | Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark |
| [https://www.arxiv.org/abs/2501.05069](https://www.arxiv.org/abs/2501.05069) | Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning |
| [https://www.arxiv.org/abs/2501.04682](https://www.arxiv.org/abs/2501.04682) | Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought |
| [https://www.arxiv.org/abs/2501.04519](https://www.arxiv.org/abs/2501.04519) | rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking |
| [https://www.arxiv.org/abs/2501.03675](https://www.arxiv.org/abs/2501.03675) | SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning |
| [https://www.arxiv.org/abs/2501.03230](https://www.arxiv.org/abs/2501.03230) | Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition |
| [https://www.arxiv.org/abs/2501.02964](https://www.arxiv.org/abs/2501.02964) | Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild |
| [https://www.arxiv.org/abs/2501.02795](https://www.arxiv.org/abs/2501.02795) | InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion |
| [https://www.arxiv.org/abs/2501.02669](https://www.arxiv.org/abs/2501.02669) | Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs? |
| [https://www.arxiv.org/abs/2501.01834](https://www.arxiv.org/abs/2501.01834) | MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning |
| [https://www.arxiv.org/abs/2501.01030](https://www.arxiv.org/abs/2501.01030) | Reasoning based on symbolic and parametric knowledge bases: a survey |
| [https://www.arxiv.org/abs/2412.21187](https://www.arxiv.org/abs/2412.21187) | Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs |
| [https://www.arxiv.org/abs/2412.20993](https://www.arxiv.org/abs/2412.20993) | Efficiently Serving LLM Reasoning Programs with Certaindex |
| [https://www.arxiv.org/abs/2412.18547](https://www.arxiv.org/abs/2412.18547) | Token-Budget-Aware LLM Reasoning |
| [https://www.arxiv.org/abs/2412.18319](https://www.arxiv.org/abs/2412.18319) | Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search |
| [https://www.arxiv.org/abs/2412.18072](https://www.arxiv.org/abs/2412.18072) | MMFactory: A Universal Solution Search Engine for Vision-Language Tasks |
| [https://www.arxiv.org/abs/2412.17498](https://www.arxiv.org/abs/2412.17498) | DRT: Deep Reasoning Translation via Long Chain-of-Thought |
| [https://www.arxiv.org/abs/2412.17451](https://www.arxiv.org/abs/2412.17451) | Diving into Self-Evolving Training for Multimodal Reasoning |
| [https://www.arxiv.org/abs/2412.15084](https://www.arxiv.org/abs/2412.15084) | AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling |
| [https://www.arxiv.org/abs/2412.14135](https://www.arxiv.org/abs/2412.14135) | Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective |
| [https://www.arxiv.org/abs/2412.13540](https://www.arxiv.org/abs/2412.13540) | Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning |
| [https://www.arxiv.org/abs/2412.13147](https://www.arxiv.org/abs/2412.13147) | Are Your LLMs Capable of Stable Reasoning? |
| [https://www.arxiv.org/abs/2412.12932](https://www.arxiv.org/abs/2412.12932) | CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models |
| [https://www.arxiv.org/abs/2412.11124](https://www.arxiv.org/abs/2412.11124) | Combating Multimodal LLM Hallucination via Bottom-Up Holistic Reasoning |
| [https://www.arxiv.org/abs/2412.11025](https://www.arxiv.org/abs/2412.11025) | From Simple to Professional: A Combinatorial Controllable Image Captioning Agent |
| [https://www.arxiv.org/abs/2412.10471](https://www.arxiv.org/abs/2412.10471) | VCA: Video Curious Agent for Long Video Understanding |
| [https://www.arxiv.org/abs/2412.10400](https://www.arxiv.org/abs/2412.10400) | Reinforcement Learning Enhanced LLMs: A Survey |
| [https://www.arxiv.org/abs/2412.09601](https://www.arxiv.org/abs/2412.09601) | TimeRefine: Temporal Grounding with Time Refining Video LLM |
| [https://www.arxiv.org/abs/2412.09413](https://www.arxiv.org/abs/2412.09413) | Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems |
| [https://www.arxiv.org/abs/2412.08859](https://www.arxiv.org/abs/2412.08859) | ViUniT: Visual Unit Tests for More Robust Visual Programming |
| [https://www.arxiv.org/abs/2412.08635](https://www.arxiv.org/abs/2412.08635) | Multimodal Latent Language Modeling with Next-Token Diffusion |
| [https://www.arxiv.org/abs/2412.08564](https://www.arxiv.org/abs/2412.08564) | Template-Based Visual Program Distillation |
| [https://www.arxiv.org/abs/2412.07148](https://www.arxiv.org/abs/2412.07148) | MM-PoE: Multiple Choice Reasoning via. Process of Elimination using Multi-Modal Models |
| [https://www.arxiv.org/abs/2412.07012](https://www.arxiv.org/abs/2412.07012) | ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models |
| [https://www.arxiv.org/abs/2412.05479](https://www.arxiv.org/abs/2412.05479) | TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action |
| [https://www.arxiv.org/abs/2412.05243](https://www.arxiv.org/abs/2412.05243) | CompCap: Improving Multimodal Large Language Models with Composite Captions |
| [https://www.arxiv.org/abs/2412.04903](https://www.arxiv.org/abs/2412.04903) | EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation |
| [https://www.arxiv.org/abs/2412.04531](https://www.arxiv.org/abs/2412.04531) | MageBench: Bridging Large Multimodal Models to Agents |
| [https://www.arxiv.org/abs/2412.03704](https://www.arxiv.org/abs/2412.03704) | Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension |
| [https://www.arxiv.org/abs/2412.03548](https://www.arxiv.org/abs/2412.03548) | Perception Tokens Enhance Visual Reasoning in Multimodal Language Models |
| [https://www.arxiv.org/abs/2412.02172](https://www.arxiv.org/abs/2412.02172) | VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning |
| [https://www.arxiv.org/abs/2412.02071](https://www.arxiv.org/abs/2412.02071) | Progress-Aware Video Frame Captioning |
| [https://www.arxiv.org/abs/2412.01694](https://www.arxiv.org/abs/2412.01694) | Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation |
| [https://www.arxiv.org/abs/2411.19943](https://www.arxiv.org/abs/2411.19943) | Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability |
| [https://www.arxiv.org/abs/2411.17451](https://www.arxiv.org/abs/2411.17451) | VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models |
| [https://www.arxiv.org/abs/2411.14405](https://www.arxiv.org/abs/2411.14405) | Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions |
| [https://www.arxiv.org/abs/2411.14251](https://www.arxiv.org/abs/2411.14251) | Natural Language Reinforcement Learning |
| [https://www.arxiv.org/abs/2411.12591](https://www.arxiv.org/abs/2411.12591) | Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination |
| [https://www.arxiv.org/abs/2411.11694](https://www.arxiv.org/abs/2411.11694) | Enhancing LLM Reasoning with Reward-guided Tree Search |
| [https://www.arxiv.org/abs/2411.10442](https://www.arxiv.org/abs/2411.10442) | Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization |
| [https://www.arxiv.org/abs/2411.10440](https://www.arxiv.org/abs/2411.10440) | LLaVA-CoT: Let Vision Language Models Reason Step-by-Step |
| [https://www.arxiv.org/abs/2411.07279](https://www.arxiv.org/abs/2411.07279) | The Surprising Effectiveness of Test-Time Training for Abstract Reasoning |
| [https://www.arxiv.org/abs/2411.00855](https://www.arxiv.org/abs/2411.00855) | Vision-Language Models Can Self-Improve Reasoning via Reflection |
| [https://www.arxiv.org/abs/2411.00238](https://www.arxiv.org/abs/2411.00238) | Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem |
| [https://www.arxiv.org/abs/2410.22315](https://www.arxiv.org/abs/2410.22315) | Natural Language Inference Improves Compositionality in Vision-Language Models |
| [https://www.arxiv.org/abs/2410.22312](https://www.arxiv.org/abs/2410.22312) | Effective Guidance for Model Attention with Simple Yes-no Annotations |
| [https://www.arxiv.org/abs/2410.21252](https://www.arxiv.org/abs/2410.21252) | LongReward: Improving Long-context Large Language Models with AI Feedback |
| [https://www.arxiv.org/abs/2410.20285](https://www.arxiv.org/abs/2410.20285) | SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement |
| [https://www.arxiv.org/abs/2410.17885](https://www.arxiv.org/abs/2410.17885) | R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models |
| [https://www.arxiv.org/abs/2410.17131](https://www.arxiv.org/abs/2410.17131) | Aligning Large Language Models via Self-Steering Optimization |
| [https://www.arxiv.org/abs/2410.16198](https://www.arxiv.org/abs/2410.16198) | Improve Vision Language Model Chain-of-thought Reasoning |
| [https://www.arxiv.org/abs/2410.13826](https://www.arxiv.org/abs/2410.13826) | Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models |
| [https://www.arxiv.org/abs/2410.13639](https://www.arxiv.org/abs/2410.13639) | A Comparative Study on Reasoning Patterns of OpenAI's o1 Model |
| [https://www.arxiv.org/abs/2410.10762](https://www.arxiv.org/abs/2410.10762) | AFlow: Automating Agentic Workflow Generation |
| [https://www.arxiv.org/abs/2410.09575](https://www.arxiv.org/abs/2410.09575) | Reconstructive Visual Instruction Tuning |
| [https://www.arxiv.org/abs/2410.09421](https://www.arxiv.org/abs/2410.09421) | VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment |
| [https://www.arxiv.org/abs/2410.08209](https://www.arxiv.org/abs/2410.08209) | Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision |
| [https://www.arxiv.org/abs/2410.06508](https://www.arxiv.org/abs/2410.06508) | Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning |
| [https://www.arxiv.org/abs/2410.04734](https://www.arxiv.org/abs/2410.04734) | TLDR: Token-Level Detective Reward Model for Large Vision Language Models |
| [https://www.arxiv.org/abs/2410.04659](https://www.arxiv.org/abs/2410.04659) | ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2410.03577](https://www.arxiv.org/abs/2410.03577) | Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2410.03321](https://www.arxiv.org/abs/2410.03321) | Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning |
| [https://www.arxiv.org/abs/2410.02884](https://www.arxiv.org/abs/2410.02884) | LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning |
| [https://www.arxiv.org/abs/2410.02768](https://www.arxiv.org/abs/2410.02768) | BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering |
| [https://www.arxiv.org/abs/2410.02724](https://www.arxiv.org/abs/2410.02724) | Large Language Models as Markov Chains |
| [https://www.arxiv.org/abs/2410.02712](https://www.arxiv.org/abs/2410.02712) | LLaVA-Critic: Learning to Evaluate Multimodal Models |
| [https://www.arxiv.org/abs/2410.01707](https://www.arxiv.org/abs/2410.01707) | Interpretable Contrastive Monte Carlo Tree Search Reasoning |
| [https://www.arxiv.org/abs/2409.19924](https://www.arxiv.org/abs/2409.19924) | On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability |
| [https://www.arxiv.org/abs/2409.19339](https://www.arxiv.org/abs/2409.19339) | Visual Question Decomposition on Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2409.14750](https://www.arxiv.org/abs/2409.14750) | FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension |
| [https://www.arxiv.org/abs/2409.12953](https://www.arxiv.org/abs/2409.12953) | JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images |
| [https://www.arxiv.org/abs/2409.12917](https://www.arxiv.org/abs/2409.12917) | Training Language Models to Self-Correct via Reinforcement Learning |
| [https://www.arxiv.org/abs/2409.12917](https://www.arxiv.org/abs/2409.12917) | Training Language Models to Self-Correct via Reinforcement Learning |
| [https://www.arxiv.org/abs/2409.08202](https://www.arxiv.org/abs/2409.08202) | What Makes a Maze Look Like a Maze? |
| [https://www.arxiv.org/abs/2409.04057](https://www.arxiv.org/abs/2409.04057) | Self-Harmonized Chain of Thought |
| [https://www.arxiv.org/abs/2408.17150](https://www.arxiv.org/abs/2408.17150) | Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning |
| [https://www.arxiv.org/abs/2408.15556](https://www.arxiv.org/abs/2408.15556) | Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2408.14469](https://www.arxiv.org/abs/2408.14469) | Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos |
| [https://www.arxiv.org/abs/2408.13890](https://www.arxiv.org/abs/2408.13890) | Making Large Language Models Better Planners with Reasoning-Decision Alignment |
| [https://www.arxiv.org/abs/2408.11813](https://www.arxiv.org/abs/2408.11813) | SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs |
| [https://www.arxiv.org/abs/2408.10433](https://www.arxiv.org/abs/2408.10433) | CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs |
| [https://www.arxiv.org/abs/2408.08632](https://www.arxiv.org/abs/2408.08632) | A Survey on Benchmarks of Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2408.07199](https://www.arxiv.org/abs/2408.07199) | Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents |
| [https://www.arxiv.org/abs/2408.06195](https://www.arxiv.org/abs/2408.06195) | Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers |
| [https://www.arxiv.org/abs/2408.05019](https://www.arxiv.org/abs/2408.05019) | Instruction Tuning-free Visual Token Complement for Multimodal LLMs |
| [https://www.arxiv.org/abs/2408.03940](https://www.arxiv.org/abs/2408.03940) | How Well Can Vision Language Models See Image Details? |
| [https://www.arxiv.org/abs/2408.03314](https://www.arxiv.org/abs/2408.03314) | Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters |
| [https://www.arxiv.org/abs/2408.02034](https://www.arxiv.org/abs/2408.02034) | Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid |
| [https://www.arxiv.org/abs/2408.02032](https://www.arxiv.org/abs/2408.02032) | Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models |
| [https://www.arxiv.org/abs/2408.00765](https://www.arxiv.org/abs/2408.00765) | MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities |
| [https://www.arxiv.org/abs/2407.21787](https://www.arxiv.org/abs/2407.21787) | Large Language Monkeys: Scaling Inference Compute with Repeated Sampling |
| [https://www.arxiv.org/abs/2407.17453](https://www.arxiv.org/abs/2407.17453) | VILA$^2$: VILA Augmented VILA |
| [https://www.arxiv.org/abs/2407.11522](https://www.arxiv.org/abs/2407.11522) | FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models |
| [https://www.arxiv.org/abs/2407.11511](https://www.arxiv.org/abs/2407.11511) | Reasoning with Large Language Models, a Survey |
| [https://www.arxiv.org/abs/2407.11422](https://www.arxiv.org/abs/2407.11422) | Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models |
| [https://www.arxiv.org/abs/2407.06189](https://www.arxiv.org/abs/2407.06189) | Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision |
| [https://www.arxiv.org/abs/2407.04973](https://www.arxiv.org/abs/2407.04973) | LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts |
| [https://www.arxiv.org/abs/2407.04681](https://www.arxiv.org/abs/2407.04681) | Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge |
| [https://www.arxiv.org/abs/2407.03008](https://www.arxiv.org/abs/2407.03008) | Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video Question-Answering |
| [https://www.arxiv.org/abs/2406.19934](https://www.arxiv.org/abs/2406.19934) | From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis |
| [https://www.arxiv.org/abs/2406.19392](https://www.arxiv.org/abs/2406.19392) | ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos |
| [https://www.arxiv.org/abs/2406.18227](https://www.arxiv.org/abs/2406.18227) | GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension |
| [https://www.arxiv.org/abs/2406.16620](https://www.arxiv.org/abs/2406.16620) | OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer |
| [https://www.arxiv.org/abs/2406.12050](https://www.arxiv.org/abs/2406.12050) | Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning |
| [https://www.arxiv.org/abs/2406.11303](https://www.arxiv.org/abs/2406.11303) | VideoVista: A Versatile Benchmark for Video Understanding and Reasoning |
| [https://www.arxiv.org/abs/2406.10923](https://www.arxiv.org/abs/2406.10923) | Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies |
| [https://www.arxiv.org/abs/2406.09390](https://www.arxiv.org/abs/2406.09390) | LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living |
| [https://www.arxiv.org/abs/2406.09279](https://www.arxiv.org/abs/2406.09279) | Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback |
| [https://www.arxiv.org/abs/2406.09175](https://www.arxiv.org/abs/2406.09175) | ReMI: A Dataset for Reasoning with Multiple Images |
| [https://www.arxiv.org/abs/2406.08164](https://www.arxiv.org/abs/2406.08164) | ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs |
| [https://www.arxiv.org/abs/2406.07394](https://www.arxiv.org/abs/2406.07394) | Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B |
| [https://www.arxiv.org/abs/2406.06592](https://www.arxiv.org/abs/2406.06592) | Improve Mathematical Reasoning in Language Models by Automated Process Supervision |
| [https://www.arxiv.org/abs/2406.03441](https://www.arxiv.org/abs/2406.03441) | Cycles of Thought: Measuring LLM Confidence through Stable Explanations |
| [https://www.arxiv.org/abs/2406.00645](https://www.arxiv.org/abs/2406.00645) | FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning |
| [https://www.arxiv.org/abs/2405.19209](https://www.arxiv.org/abs/2405.19209) | VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos |
| [https://www.arxiv.org/abs/2405.16473](https://www.arxiv.org/abs/2405.16473) | M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought |
| [https://www.arxiv.org/abs/2405.16071](https://www.arxiv.org/abs/2405.16071) | DynRefer: Delving into Region-level Multimodal Tasks via Dynamic Resolution |
| [https://www.arxiv.org/abs/2405.14205](https://www.arxiv.org/abs/2405.14205) | Agent Planning with World Knowledge Model |
| [https://www.arxiv.org/abs/2405.13872](https://www.arxiv.org/abs/2405.13872) | Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2405.09711](https://www.arxiv.org/abs/2405.09711) | STAR: A Benchmark for Situated Reasoning in Real-World Videos |
| [https://www.arxiv.org/abs/2405.03272](https://www.arxiv.org/abs/2405.03272) | WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning |
| [https://www.arxiv.org/abs/2405.00451](https://www.arxiv.org/abs/2405.00451) | Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning |
| [https://www.arxiv.org/abs/2404.18033](https://www.arxiv.org/abs/2404.18033) | Exposing Text-Image Inconsistency Using Diffusion Models |
| [https://www.arxiv.org/abs/2404.16222](https://www.arxiv.org/abs/2404.16222) | Step Differences in Instructional Video |
| [https://www.arxiv.org/abs/2404.15190](https://www.arxiv.org/abs/2404.15190) | Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following |
| [https://www.arxiv.org/abs/2404.13847](https://www.arxiv.org/abs/2404.13847) | EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning |
| [https://www.arxiv.org/abs/2404.12390](https://www.arxiv.org/abs/2404.12390) | BLINK: Multimodal Large Language Models Can See but Not Perceive |
| [https://www.arxiv.org/abs/2404.12358](https://www.arxiv.org/abs/2404.12358) | From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function |
| [https://www.arxiv.org/abs/2404.09797](https://www.arxiv.org/abs/2404.09797) | TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding |
| [https://www.arxiv.org/abs/2404.08471](https://www.arxiv.org/abs/2404.08471) | Revisiting Feature Prediction for Learning Visual Representations from Video |
| [https://www.arxiv.org/abs/2404.07449](https://www.arxiv.org/abs/2404.07449) | Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs |
| [https://www.arxiv.org/abs/2404.06511](https://www.arxiv.org/abs/2404.06511) | MoReVQA: Exploring Modular Reasoning Models for Video Question Answering |
| [https://www.arxiv.org/abs/2404.06510](https://www.arxiv.org/abs/2404.06510) | Can Feedback Enhance Semantic Grounding in Large Vision-Language Models? |
| [https://www.arxiv.org/abs/2404.04007](https://www.arxiv.org/abs/2404.04007) | Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering |
| [https://www.arxiv.org/abs/2404.01911](https://www.arxiv.org/abs/2404.01911) | VLRM: Vision-Language Models act as Reward Models for Image Captioning |
| [https://www.arxiv.org/abs/2404.01869](https://www.arxiv.org/abs/2404.01869) | Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey |
| [https://www.arxiv.org/abs/2404.01299](https://www.arxiv.org/abs/2404.01299) | CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes |
| [https://www.arxiv.org/abs/2404.01258](https://www.arxiv.org/abs/2404.01258) | Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward |
| [https://www.arxiv.org/abs/2404.01230](https://www.arxiv.org/abs/2404.01230) | LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models |
| [https://www.arxiv.org/abs/2404.00860](https://www.arxiv.org/abs/2404.00860) | Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance |
| [https://www.arxiv.org/abs/2403.19322](https://www.arxiv.org/abs/2403.19322) | Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2403.16999](https://www.arxiv.org/abs/2403.16999) | Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning |
| [https://www.arxiv.org/abs/2403.16921](https://www.arxiv.org/abs/2403.16921) | PropTest: Automatic Property Testing for Improved Visual Programming |
| [https://www.arxiv.org/abs/2403.14743](https://www.arxiv.org/abs/2403.14743) | VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding |
| [https://www.arxiv.org/abs/2403.12966](https://www.arxiv.org/abs/2403.12966) | Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models |
| [https://www.arxiv.org/abs/2403.12884](https://www.arxiv.org/abs/2403.12884) | HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning |
| [https://www.arxiv.org/abs/2403.11085](https://www.arxiv.org/abs/2403.11085) | m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks |
| [https://www.arxiv.org/abs/2403.10191](https://www.arxiv.org/abs/2403.10191) | Generative Region-Language Pretraining for Open-Ended Object Detection |
| [https://www.arxiv.org/abs/2403.09629](https://www.arxiv.org/abs/2403.09629) | Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking |
| [https://www.arxiv.org/abs/2403.09394](https://www.arxiv.org/abs/2403.09394) | GiT: Towards Generalist Vision Transformer through Universal Language Interface |
| [https://www.arxiv.org/abs/2403.09333](https://www.arxiv.org/abs/2403.09333) | Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring |
| [https://www.arxiv.org/abs/2403.07487](https://www.arxiv.org/abs/2403.07487) | Motion Mamba: Efficient and Long Sequence Motion Generation |
| [https://www.arxiv.org/abs/2403.02969](https://www.arxiv.org/abs/2403.02969) | Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception |
| [https://www.arxiv.org/abs/2402.19467](https://www.arxiv.org/abs/2402.19467) | TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning |
| [https://www.arxiv.org/abs/2402.19405](https://www.arxiv.org/abs/2402.19405) | Navigating Hallucinations for Reasoning of Unintentional Activities |
| [https://www.arxiv.org/abs/2402.16837](https://www.arxiv.org/abs/2402.16837) | Do Large Language Models Latently Perform Multi-Hop Reasoning? |
| [https://www.arxiv.org/abs/2402.15527](https://www.arxiv.org/abs/2402.15527) | PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain |
| [https://www.arxiv.org/abs/2402.15300](https://www.arxiv.org/abs/2402.15300) | Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding |
| [https://www.arxiv.org/abs/2402.14899](https://www.arxiv.org/abs/2402.14899) | Stop Reasoning! When Multimodal LLM with Chain-of-Thought Reasoning Meets Adversarial Image |
| [https://www.arxiv.org/abs/2402.14767](https://www.arxiv.org/abs/2402.14767) | DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models |
| [https://www.arxiv.org/abs/2402.14683](https://www.arxiv.org/abs/2402.14683) | Visual Hallucinations of Multi-modal Large Language Models |
| [https://www.arxiv.org/abs/2402.14545](https://www.arxiv.org/abs/2402.14545) | Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective |
| [https://www.arxiv.org/abs/2402.13950](https://www.arxiv.org/abs/2402.13950) | Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning |
| [https://www.arxiv.org/abs/2402.13904](https://www.arxiv.org/abs/2402.13904) | Calibrating Large Language Models with Sample Consistency |
| [https://www.arxiv.org/abs/2402.13254](https://www.arxiv.org/abs/2402.13254) | CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples |
| [https://www.arxiv.org/abs/2402.13212](https://www.arxiv.org/abs/2402.13212) | Soft Self-Consistency Improves Language Model Agents |
| [https://www.arxiv.org/abs/2402.11622](https://www.arxiv.org/abs/2402.11622) | Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models |
| [https://www.arxiv.org/abs/2402.10200](https://www.arxiv.org/abs/2402.10200) | Chain-of-Thought Reasoning Without Prompting |
| [https://www.arxiv.org/abs/2402.07384](https://www.arxiv.org/abs/2402.07384) | Exploring Perceptual Limitation of Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2402.07087](https://www.arxiv.org/abs/2402.07087) | Self-Correcting Self-Consuming Loops for Generative Model Training |
| [https://www.arxiv.org/abs/2402.06457](https://www.arxiv.org/abs/2402.06457) | V-STaR: Training Verifiers for Self-Taught Reasoners |
| [https://www.arxiv.org/abs/2402.06118](https://www.arxiv.org/abs/2402.06118) | ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling |
| [https://www.arxiv.org/abs/2402.04788](https://www.arxiv.org/abs/2402.04788) | MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark |
| [https://www.arxiv.org/abs/2402.04236](https://www.arxiv.org/abs/2402.04236) | CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning |
| [https://www.arxiv.org/abs/2402.01345](https://www.arxiv.org/abs/2402.01345) | Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models |
| [https://www.arxiv.org/abs/2402.00782](https://www.arxiv.org/abs/2402.00782) | Dense Reward for Free in Reinforcement Learning from Human Feedback |
| [https://www.arxiv.org/abs/2401.17981](https://www.arxiv.org/abs/2401.17981) | From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information |
| [https://www.arxiv.org/abs/2401.13311](https://www.arxiv.org/abs/2401.13311) | ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models |
| [https://www.arxiv.org/abs/2401.13011](https://www.arxiv.org/abs/2401.13011) | CCA: Collaborative Competitive Agents for Image Editing |
| [https://www.arxiv.org/abs/2401.09865](https://www.arxiv.org/abs/2401.09865) | Improving fine-grained understanding in image-text pre-training |
| [https://www.arxiv.org/abs/2401.08541](https://www.arxiv.org/abs/2401.08541) | Scalable Pre-training of Large Autoregressive Image Models |
| [https://www.arxiv.org/abs/2401.07529](https://www.arxiv.org/abs/2401.07529) | MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception |
| [https://www.arxiv.org/abs/2401.03105](https://www.arxiv.org/abs/2401.03105) | Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2312.17240](https://www.arxiv.org/abs/2312.17240) | LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model |
| [https://www.arxiv.org/abs/2312.17172](https://www.arxiv.org/abs/2312.17172) | Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action |
| [https://www.arxiv.org/abs/2312.15162](https://www.arxiv.org/abs/2312.15162) | Cycle-Consistency Learning for Captioning and Grounding |
| [https://www.arxiv.org/abs/2312.14135](https://www.arxiv.org/abs/2312.14135) | V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs |
| [https://www.arxiv.org/abs/2312.12423](https://www.arxiv.org/abs/2312.12423) | Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model |
| [https://www.arxiv.org/abs/2312.11562](https://www.arxiv.org/abs/2312.11562) | A Survey of Reasoning with Foundation Models |
| [https://www.arxiv.org/abs/2312.11420](https://www.arxiv.org/abs/2312.11420) | Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning |
| [https://www.arxiv.org/abs/2312.10807](https://www.arxiv.org/abs/2312.10807) | Bridging Language and Action: A Survey of Language-Conditioned Robot Manipulation |
| [https://www.arxiv.org/abs/2312.10665](https://www.arxiv.org/abs/2312.10665) | Silkie: Preference Distillation for Large Visual Language Models |
| [https://www.arxiv.org/abs/2312.09337](https://www.arxiv.org/abs/2312.09337) | Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences |
| [https://www.arxiv.org/abs/2312.09251](https://www.arxiv.org/abs/2312.09251) | VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation |
| [https://www.arxiv.org/abs/2312.09238](https://www.arxiv.org/abs/2312.09238) | Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft |
| [https://www.arxiv.org/abs/2312.08870](https://www.arxiv.org/abs/2312.08870) | Vista-LLaMA: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens |
| [https://www.arxiv.org/abs/2312.07532](https://www.arxiv.org/abs/2312.07532) | Interfacing Foundation Models' Embeddings |
| [https://www.arxiv.org/abs/2312.05230](https://www.arxiv.org/abs/2312.05230) | Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning |
| [https://www.arxiv.org/abs/2312.03631](https://www.arxiv.org/abs/2312.03631) | Mitigating Open-Vocabulary Caption Hallucinations |
| [https://www.arxiv.org/abs/2312.03052](https://www.arxiv.org/abs/2312.03052) | Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models |
| [https://www.arxiv.org/abs/2312.02949](https://www.arxiv.org/abs/2312.02949) | LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models |
| [https://www.arxiv.org/abs/2311.17902](https://www.arxiv.org/abs/2311.17902) | Language-conditioned Detection Transformer |
| [https://www.arxiv.org/abs/2311.16922](https://www.arxiv.org/abs/2311.16922) | Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding |
| [https://www.arxiv.org/abs/2311.13601](https://www.arxiv.org/abs/2311.13601) | Visual In-Context Prompting |
| [https://www.arxiv.org/abs/2311.07362](https://www.arxiv.org/abs/2311.07362) | Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision |
| [https://www.arxiv.org/abs/2311.06612](https://www.arxiv.org/abs/2311.06612) | PerceptionGPT: Effectively Fusing Visual Perception into LLM |
| [https://www.arxiv.org/abs/2311.03079](https://www.arxiv.org/abs/2311.03079) | CogVLM: Visual Expert for Pretrained Language Models |
| [https://www.arxiv.org/abs/2311.00233](https://www.arxiv.org/abs/2311.00233) | Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions |
| [https://www.arxiv.org/abs/2310.19785](https://www.arxiv.org/abs/2310.19785) | What's "up" with vision-language models? Investigating their struggle with spatial reasoning |
| [https://www.arxiv.org/abs/2310.15100](https://www.arxiv.org/abs/2310.15100) | LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis |
| [https://www.arxiv.org/abs/2310.11511](https://www.arxiv.org/abs/2310.11511) | Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection |
| [https://www.arxiv.org/abs/2310.10080](https://www.arxiv.org/abs/2310.10080) | Let's reward step by step: Step-Level reward model as the Navigators for Reasoning |
| [https://www.arxiv.org/abs/2310.07932](https://www.arxiv.org/abs/2310.07932) | What Matters to You? Towards Visual Representation Alignment for Robot Learning |
| [https://www.arxiv.org/abs/2310.06627](https://www.arxiv.org/abs/2310.06627) | What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models |
| [https://www.arxiv.org/abs/2310.06271](https://www.arxiv.org/abs/2310.06271) | Towards Mitigating Hallucination in Large Language Models via Self-Reflection |
| [https://www.arxiv.org/abs/2310.04406](https://www.arxiv.org/abs/2310.04406) | Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models |
| [https://www.arxiv.org/abs/2309.17400](https://www.arxiv.org/abs/2309.17400) | Directly Fine-Tuning Diffusion Models on Differentiable Rewards |
| [https://www.arxiv.org/abs/2309.17179](https://www.arxiv.org/abs/2309.17179) | Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training |
| [https://www.arxiv.org/abs/2309.15112](https://www.arxiv.org/abs/2309.15112) | InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition |
| [https://www.arxiv.org/abs/2309.14525](https://www.arxiv.org/abs/2309.14525) | Aligning Large Multimodal Models with Factually Augmented RLHF |
| [https://www.arxiv.org/abs/2309.11489](https://www.arxiv.org/abs/2309.11489) | Text2Reward: Reward Shaping with Language Models for Reinforcement Learning |
| [https://www.arxiv.org/abs/2309.10790](https://www.arxiv.org/abs/2309.10790) | Guide Your Agent with Adaptive Multimodal Rewards |
| [https://www.arxiv.org/abs/2309.07124](https://www.arxiv.org/abs/2309.07124) | RAIN: Your Language Models Can Align Themselves without Finetuning |
| [https://www.arxiv.org/abs/2309.03409](https://www.arxiv.org/abs/2309.03409) | Large Language Models as Optimizers |
| [https://www.arxiv.org/abs/2308.13437](https://www.arxiv.org/abs/2308.13437) | Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models |
| [https://www.arxiv.org/abs/2308.11662](https://www.arxiv.org/abs/2308.11662) | VQA Therapy: Exploring Answer Differences by Visually Grounding Answers |
| [https://www.arxiv.org/abs/2308.01825](https://www.arxiv.org/abs/2308.01825) | Scaling Relationship on Learning Mathematical Reasoning with Large Language Models |
| [https://www.arxiv.org/abs/2307.05222](https://www.arxiv.org/abs/2307.05222) | Emu: Generative Pretraining in Multimodality |
| [https://www.arxiv.org/abs/2306.08129](https://www.arxiv.org/abs/2306.08129) | AVIS: Autonomous Visual Information Seeking with Large Language Model Agent |
| [https://www.arxiv.org/abs/2305.20050](https://www.arxiv.org/abs/2305.20050) | Let's Verify Step by Step |
| [https://www.arxiv.org/abs/2305.14167](https://www.arxiv.org/abs/2305.14167) | DetGPT: Detect What You Need via Reasoning |
| [https://www.arxiv.org/abs/2305.00633](https://www.arxiv.org/abs/2305.00633) | Self-Evaluation Guided Beam Search for Reasoning |
| [https://www.arxiv.org/abs/2303.17651](https://www.arxiv.org/abs/2303.17651) | Self-Refine: Iterative Refinement with Self-Feedback |
| [https://www.arxiv.org/abs/2210.01241](https://www.arxiv.org/abs/2210.01241) | Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization |
| [https://www.arxiv.org/abs/2203.14465](https://www.arxiv.org/abs/2203.14465) | STaR: Bootstrapping Reasoning With Reasoning |
| [https://www.arxiv.org/abs/2203.11171](https://www.arxiv.org/abs/2203.11171) | Self-Consistency Improves Chain of Thought Reasoning in Language Models |
| [https://www.arxiv.org/abs/2104.05218](https://www.arxiv.org/abs/2104.05218) | FUDGE: Controlled Text Generation With Future Discriminators |

