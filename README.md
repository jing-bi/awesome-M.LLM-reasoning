<h1 align="center" style="font-size: 48px; font-weight: 700; margin: 0;">
  üìñ Why Reasoning Matters?
</h1>
<h3 align="center" style="font-size: 28px; font-weight: 500; color: #555; margin: 0;">
  A Survey of Advancements in Multimodal Reasoning
</h3>

## üî• All You Need for Reasoning Papers  [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)


Fresh Papers, Every Day: We track arXiv and top conferences so you don‚Äôt have to. New papers are added daily ‚Äî check back often or follow along to stay in the loop.

üåê Web Interface (Coming Soon): We're building a powerful, user-friendly platform to help you **filter**, **explore**, and **stay focused** on the research that matters most to you.


## üì∞ Table of Contents
- [Background](#-background)
- [Paper List](#-paper-list)
- [Star History](#-star-history)
- [Citation](#%EF%B8%8F-citation)


## üéâ Background
With the rapid advancement of Multimodal Large Language Models (MLLMs), their reasoning capabilities have become a focal point of research. Understanding how these models process and integrate information across different modalities is essential for improving interpretability and reliability. These developments have led to an increasing body of work analyzing the fundamental reasoning capabilities and Aha Moments within LMLMs, which we refer to as Multimodal Reasoning Paradigms.


<img src="assets\search_framework.png" alt="search framework" width="800">


## üìö Paper List

### Categories

- [üî• All You Need for Reasoning Papers  ](#-all-you-need-for-reasoning-papers--)
- [üì∞ Table of Contents](#-table-of-contents)
- [üéâ Background](#-background)
- [‚úèÔ∏èCitation](#%EF%B8%8F-citation)
- [üìö Paper List](#-paper-list)
  - [Categories](#categories)
  - [Benchmark/Dataset ](#benchmarkdataset-)
  - [Method ](#method-)
  - [Theory ](#theory-)
  - [Application/System ](#applicationsystem-)
  - [Survey ](#survey-)
  - [Analysis/Evaluation ](#analysisevaluation-)
- [üåü Star History](#-star-history)

### Benchmark/Dataset <a id='benchmark-dataset'></a>

<table style="width: 100%;">
<tr>
<td><strong>Date</strong></td>
<td><strong>Title & Authors</strong></td>
<td><strong>Tags</strong></td>
<td><strong>MLLM Type</strong></td>
<td><strong>Links</strong></td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</strong><br><em>Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant√© Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2210.01241"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/allenai/rl4lms"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2210.01241"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Let's Verify Step by Step</strong><br><em>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2305.20050"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/openai/prm800k"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2305.20050"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VQA Therapy: Exploring Answer Differences by Visually Grounding Answers</strong><br><em>Chongyan Chen, Samreen Anjum, Danna Gurari</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2308.11662"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ccychongyanchen/vqatherapycrowdsourcing"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2308.11662"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Aligning Large Multimodal Models with Factually Augmented RLHF</strong><br><em>Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, Trevor Darrell</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.14525"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2309.14525"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</strong><br><em>Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, Bingchen Zhao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.06627"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/letian2003/c-vqa"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>What's "up" with vision-language models? Investigating their struggle with spatial reasoning</strong><br><em>Amita Kamath, Jack Hessel, Kai-Wei Chang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.19785"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/amitakamath/whatsup_vlms"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2310.19785"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</strong><br><em>Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, Jianwei Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.02949"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ux-decoder/llava-grounding"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.02949"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Mitigating Open-Vocabulary Caption Hallucinations</strong><br><em>Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/captioning-blue" alt="captioning Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.03631"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/assafbk/mocha_code"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.03631"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Interfacing Foundation Models' Embeddings</strong><br><em>Xueyan Zou, Linjie Li, Jianfeng Wang, Jianwei Yang, Mingyu Ding, Feng Li, Hao Zhang, Shilong Liu, Arul Aravinthan, Yong Jae Lee, Lijuan Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/segmentation-blue" alt="segmentation Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.07532"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ux-decoder/find"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.07532"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs</strong><br><em>Penghao Wu, Saining Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/high resolution-blue" alt="high resolution Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.14135"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/penghao-wu/vstar"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.14135"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception</strong><br><em>Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, Yanfeng Wang, Vivek et al.</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.07529"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/yhwmz/mm-sap"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2401.07529"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Scalable Pre-training of Large Autoregressive Image Models</strong><br><em>Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua M Susskind, Armand Joulin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.08541"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/apple/ml-aim"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2401.08541"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models</strong><br><em>Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, Nanyun Peng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.13311"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/rohan598/contextual"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2401.13311"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</strong><br><em>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.04788"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/dongping-chen/mllm-as-a-judge"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.04788"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling</strong><br><em>Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, Li Erran Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.06118"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/amazon-science/vigor"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.06118"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</strong><br><em>Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.13254"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hansolo9682/countercurate"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.13254"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual Hallucinations of Multi-modal Large Language Models</strong><br><em>Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.14683"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/wenhuang2000/vhtest"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models</strong><br><em>Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, Jiaqi Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.14767"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/DualFocus"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.14767"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</strong><br><em>Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.15527"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/pkunlp-icler/pca-eval"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.15527"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Navigating Hallucinations for Reasoning of Unintentional Activities</strong><br><em>Shresth Grover, Vibhav Vineet, Yogesh S Rawat</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.19405"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks</strong><br><em>Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.11085"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/RAIVNLab/mms"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.11085"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models</strong><br><em>Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.16999"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/deepcs233/visual-cot"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.16999"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models</strong><br><em>Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, Yin Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/plug and play-blue" alt="plug and play Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/high resolution-blue" alt="high resolution Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.19322"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2403.19322"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes</strong><br><em>Ting En Lam, Yuhan Chen, Elston Tan, Eric Peh, Ruirui Chen, Paritosh Parmar</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.01299"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/lunaproject22/causalchaos"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>BLINK: Multimodal Large Language Models Can See but Not Perceive</strong><br><em>Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.12390"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2404.12390"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Exposing Text-Image Inconsistency Using Diffusion Models</strong><br><em>Mingzhen Huang, Shan Jia, Zhou Zhou, Yan Ju, Jialing Cai, Siwei Lyu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/diffusion-blue" alt="diffusion Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.18033"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/Mingzhen-Huang/D-TIIL"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2404.18033"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning</strong><br><em>Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, Ziwei Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.03272"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>STAR: A Benchmark for Situated Reasoning in Real-World Videos</strong><br><em>Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B. Tenenbaum, Chuang Gan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/symbolic-blue" alt="symbolic Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.09711"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</strong><br><em>Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, Wanxiang Che</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.16473"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/LightChen233/M3CoT"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2405.16473"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs</strong><br><em>Irene Huang, Wei Lin, M. Jehanzeb Mirza, Jacob A. Hansen, Sivan Doveh, Victor Ion Butoi, Roei Herzig, Hilde Kuhene, Trevor Darrel, Chuang Gan, Aude Oliva, Rogerio Feris, Leonid Karlinsky</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/data generation-blue" alt="data generation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.08164"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/jmiemirza/conme"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.08164"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ReMI: A Dataset for Reasoning with Multiple Images</strong><br><em>Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal Awasthi, Dee Guo, Sreenivas Gollapudi, Ahmed Qureshi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.09175"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2406.09175"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living</strong><br><em>Rajatsubhra Chakraborty, Arkaprava Sinha, Dominick Reilly, Manish Kumar, Pu Wang, Francois Bremond, Srijan Das</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.09390"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies</strong><br><em>Hung-Ting Su, Chun-Tong Chao, Ya-Ching Hsu, Xudong Lin, Yulei Niu, Hung-Yi Lee, Winston H. Hsu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.10923"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VideoVista: A Versatile Benchmark for Video Understanding and Reasoning</strong><br><em>Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, Min Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/logic-blue" alt="logic Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.11303"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2406.11303"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension</strong><br><em>Jiafeng Liang, Shixin Jiang, Zekun Wang, Haojie Pan, Zerui Chen, Zheng Chu, Ming Liu, Ruiji Fu, Zhongyuan Wang, Bing Qin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/captioning-blue" alt="captioning Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.18227"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2406.18227"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos</strong><br><em>Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.19392"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/rextime/rextime"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.19392"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis</strong><br><em>Chuanqi Cheng, Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.19934"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/steven-ccq/visualreasoner"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.19934"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts</strong><br><em>Yijia Xiao, Edward Sun, Tianyu Liu, Wei Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/logic-blue" alt="logic Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.04973"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/yijia-xiao/logicvista"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2407.04973"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models</strong><br><em>Jinrui Zhang, Teng Wang, Haigang Zhang, Ping Lu, Feng Zheng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.11422"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models</strong><br><em>Pengxiang Li, Zhi Gao, Bofei Zhang, Tao Yuan, Yuwei Wu, Mehrtash Harandi, Yunde Jia, Song-Chun Zhu, Qing Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.11522"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2407.11522"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities</strong><br><em>Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, Xinchao Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.00765"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/yuweihao/mm-vet"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.00765"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>A Survey on Benchmarks of Multimodal Large Language Models</strong><br><em>Jian Li, Weiheng Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.08632"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/swordlidev/evaluation-multimodal-llms-survey"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.08632"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos</strong><br><em>Qirui Chen, Shangzhe Di, Weidi Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.14469"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2408.14469"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models</strong><br><em>Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Dacheng Tao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/high resolution-blue" alt="high resolution Badge"> <img src="https://img.shields.io/badge/training free-blue" alt="training free Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.15556"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/DreamMr/HR-Bench"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.15556"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>What Makes a Maze Look Like a Maze?</strong><br><em>Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.08202"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2409.08202"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images</strong><br><em>Zhecan Wang, Junzhang Liu, Chia-Wei Tang, Hani Alomari, Anushka Sivakumar</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.12953"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/journeybench/journeybench"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension</strong><br><em>Junzhuo Liu, Xuzheng Yang, Weiwei Li, Peng Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/referring expression-blue" alt="referring expression Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.14750"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/liujunzhuo/FineCops-Ref"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models</strong><br><em>Ziyue Wang, Chi Chen, Fuwen Luo, Yurui Dong, Yuanchi Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.04659"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/THUNLP-MT/ActiView"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.04659"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment</strong><br><em>Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.09421"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.09421"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLaVA-o1: Let Vision Language Models Reason Step-by-Step</strong><br><em>Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.10440"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/PKU-YuanGroup/LLaVA-CoT"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.10440"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization</strong><br><em>Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/distribution shift-blue" alt="distribution shift Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.10442"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2411.10442"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models</strong><br><em>Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.17451"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2411.17451"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Progress-Aware Video Frame Captioning</strong><br><em>Zihui Xue, Joungbin An, Xitong Yang, Kristen Grauman</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.02071"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning</strong><br><em>Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.02172"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.02172"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MageBench: Bridging Large Multimodal Models to Agents</strong><br><em>Miaosen Zhang, Qi Dai, Yifan Yang, Jianmin Bao, Dongdong Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.04531"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/microsoft/magebench"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CompCap: Improving Multimodal Large Language Models with Composite Captions</strong><br><em>Xiaohui Chen, Satya Narayan Shukla, Mahmoud Azab, Aashu Singh, Qifan Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.05243"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.05243"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models</strong><br><em>Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/data generation-blue" alt="data generation Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.07012"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/jieyuz2/provision"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.07012"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models</strong><br><em>Zihui Cheng, Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.12932"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/czhhzc/CoMT"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.12932"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Are Your LLMs Capable of Stable Reasoning?</strong><br><em>Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.13147"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/open-compass/gpassk"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.13147"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning</strong><br><em>Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Min Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.13540"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/aaandy-zhu/vgcure"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</strong><br><em>Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.15084"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.15084"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search</strong><br><em>Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.18319"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hjyao00/mulberry"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.18319"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?</strong><br><em>Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.02669"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/princeton-pli/vlm_s2h"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.02669"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning</strong><br><em>Andrew Li, Rahul Thapa, Rahul Chalamala, Qingyang Wu, Kezhen Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.03675"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/togethercomputer/smir"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.03675"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark</strong><br><em>Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.05444"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hychaochao/EMMA"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.05444"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>TimeLogic: A Temporal Logic Benchmark for Video QA</strong><br><em>Sirnam Swetha, Hilde Kuehne, Mubarak Shah</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/logic-blue" alt="logic Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.07214"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No!</strong><br><em>Mohamed Fazli Imam, Chenyang Lyu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.10674"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.10674"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning</strong><br><em>Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.07459"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/gersteinlab/medagents-benchmark"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.07459"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of Role-Playing Language Agents</strong><br><em>Rui Xu, MingYu Wang, XinTao Wang, Dakuan Lu, Xiaoyu Tan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/memory-blue" alt="memory Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.08193"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model</strong><br><em>Bowen Zhang, Pengcheng Luo, Pengcheng Luo</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.10009"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/bwz96sco/or_llm_agent"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes through Tree-of-Error</strong><br><em>Shu-Xun Yang, Cunxiang Wang, Yidong Wang, Xiaotao Gu, Minlie Huang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.10105"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/shu-xun/stepmathagent"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
</table>


<p align="right"><a href="#-table-of-contents">Back to Top</a></p>

### Method <a id='method'></a>

<table style="width: 100%;">
<tr>
<td><strong>Date</strong></td>
<td><strong>Title & Authors</strong></td>
<td><strong>Tags</strong></td>
<td><strong>MLLM Type</strong></td>
<td><strong>Links</strong></td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>FUDGE: Controlled Text Generation With Future Discriminators</strong><br><em>Kevin Yang, Dan Klein</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2104.05218"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/yangkevin2/naacl-2021-fudge-controlled-generation"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Self-Consistency Improves Chain of Thought Reasoning in Language Models</strong><br><em>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/self consistency-blue" alt="self consistency Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2203.11171"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2203.11171"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>STaR: Bootstrapping Reasoning With Reasoning</strong><br><em>Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/few shot-blue" alt="few shot Badge"> <img src="https://img.shields.io/badge/self training-blue" alt="self training Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2203.14465"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ezelikman/STaR"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2203.14465"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</strong><br><em>Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant√© Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2210.01241"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/allenai/rl4lms"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2210.01241"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Self-Refine: Iterative Refinement with Self-Feedback</strong><br><em>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/self refine-blue" alt="self refine Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2303.17651"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2303.17651"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Self-Evaluation Guided Beam Search for Reasoning</strong><br><em>Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/self evaluation-blue" alt="self evaluation Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2305.00633"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2305.00633"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>DetGPT: Detect What You Need via Reasoning</strong><br><em>Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/object detect-blue" alt="object detect Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2305.14167"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2305.14167"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Let's Verify Step by Step</strong><br><em>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2305.20050"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/openai/prm800k"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2305.20050"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>AVIS: Autonomous Visual Information Seeking with Large Language Model Agent</strong><br><em>Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A Ross, Cordelia Schmid, Alireza Fathi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/memory-blue" alt="memory Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2306.08129"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2306.08129"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Emu: Generative Pretraining in Multimodality</strong><br><em>Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/few shot-blue" alt="few shot Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2307.05222"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/baaivision/emu"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2307.05222"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</strong><br><em>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2308.01825"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ofa-sys/gsm8k-screl"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2308.01825"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VQA Therapy: Exploring Answer Differences by Visually Grounding Answers</strong><br><em>Chongyan Chen, Samreen Anjum, Danna Gurari</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2308.11662"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ccychongyanchen/vqatherapycrowdsourcing"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2308.11662"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models</strong><br><em>Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, Yang Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/region-blue" alt="region Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2308.13437"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/pvit-official/pvit"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2308.13437"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Large Language Models as Optimizers</strong><br><em>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.03409"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/google-deepmind/opro"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2309.03409"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>RAIN: Your Language Models Can Align Themselves without Finetuning</strong><br><em>Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/self evaluation-blue" alt="self evaluation Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.07124"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/SafeAILab/RAIN"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2309.07124"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Guide Your Agent with Adaptive Multimodal Rewards</strong><br><em>Changyeon Kim, Younggyo Seo, Hao Liu, Lisa Lee, Honglak Lee, Jinwoo Shin, Kimin Lee</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.10790"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2309.10790"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Text2Reward: Reward Shaping with Language Models for Reinforcement Learning</strong><br><em>Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/robotic-blue" alt="robotic Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.11489"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/xlang-ai/text2reward"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2309.11489"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Aligning Large Multimodal Models with Factually Augmented RLHF</strong><br><em>Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, Trevor Darrell</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.14525"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2309.14525"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</strong><br><em>Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.15112"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/internlm/internlm-xcomposer"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2309.15112"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training</strong><br><em>Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/value function-blue" alt="value function Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.17179"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/waterhorse1/llm_tree_search"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2309.17179"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Directly Fine-Tuning Diffusion Models on Differentiable Rewards</strong><br><em>Kevin Clark, Paul Vicol, Kevin Swersky, David J. Fleet</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> <img src="https://img.shields.io/badge/diffusion-blue" alt="diffusion Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.17400"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2309.17400"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models</strong><br><em>Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.04406"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/lapisrocks/languageagenttreesearch"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2310.04406"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Towards Mitigating LLM Hallucination via Self Reflection</strong><br><em>Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.06271"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2310.06271"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</strong><br><em>Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, Bingchen Zhao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.06627"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/letian2003/c-vqa"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>What Matters to You? Towards Visual Representation Alignment for Robot Learning</strong><br><em>Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/robotic-blue" alt="robotic Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.07932"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2310.07932"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Let's reward step by step: Step-Level reward model as the Navigators for Reasoning</strong><br><em>Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.10080"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2310.10080"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</strong><br><em>Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/retrieval-blue" alt="retrieval Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.11511"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/AkariAsai/self-rag"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2310.11511"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis</strong><br><em>Shih-Chieh Dai, Aiping Xiong, Lun-Wei Ku</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.15100"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/sjdai/llm-thematic-analysis"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions</strong><br><em>Taehyeon Kim, Joonkee Kim, Gihun Lee, Se-Young Yun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2311.00233"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/joonkeekim/Instructive-Decoding"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2311.00233"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CogVLM: Visual Expert for Pretrained Language Models</strong><br><em>Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Keqin Chen, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2311.03079"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/thudm/cogvlm"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2311.03079"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>PerceptionGPT: Effectively Fusing Visual Perception into LLM</strong><br><em>Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, Tong Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/efficient-blue" alt="efficient Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2311.06612"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision</strong><br><em>Seongyun Lee, Sue Hyun Park, Yongrae Jo, Minjoon Seo</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2311.07362"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/kaistai/volcano"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2311.07362"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual In-Context Prompting</strong><br><em>Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/segmentation-blue" alt="segmentation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2311.13601"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ux-decoder/dinov"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2311.13601"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding</strong><br><em>Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, Lidong Bing</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> <img src="https://img.shields.io/badge/training free-blue" alt="training free Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2311.16922"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/bradyfu/awesome-multimodal-large-language-models"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2311.16922"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Language-conditioned Detection Transformer</strong><br><em>Jang Hyun Cho, Philipp Kr√§henb√ºhl</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/object detect-blue" alt="object detect Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2311.17902"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/janghyuncho/decola"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models</strong><br><em>Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/retrieval-blue" alt="retrieval Badge"> <img src="https://img.shields.io/badge/distillation-blue" alt="distillation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.03052"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.03052"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Mitigating Open-Vocabulary Caption Hallucinations</strong><br><em>Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/captioning-blue" alt="captioning Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.03631"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/assafbk/mocha_code"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.03631"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Interfacing Foundation Models' Embeddings</strong><br><em>Xueyan Zou, Linjie Li, Jianfeng Wang, Jianwei Yang, Mingyu Ding, Feng Li, Hao Zhang, Shilong Liu, Arul Aravinthan, Yong Jae Lee, Lijuan Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/segmentation-blue" alt="segmentation Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.07532"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ux-decoder/find"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.07532"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens</strong><br><em>Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, Yi Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/attention-blue" alt="attention Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.08870"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</strong><br><em>Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.09238"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.09238"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation</strong><br><em>Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/few shot-blue" alt="few shot Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.09251"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ailab-cvc/vl-gpt"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.09251"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences</strong><br><em>Minyoung Hwang, Luca Weihs, Chanwoo Park, Kimin Lee, Aniruddha Kembhavi, Kiana Ehsani</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/embodied-blue" alt="embodied Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/robotic-blue" alt="robotic Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.09337"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Silkie: Preference Distillation for Large Visual Language Models</strong><br><em>Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.10665"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.10665"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning</strong><br><em>Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, Cihang Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/attention-blue" alt="attention Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.11420"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.11420"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Jack of All Tasks, Master of Many_ Designing General-purpose Coarse-to-Fine Vision-Language Model</strong><br><em>Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/segmentation-blue" alt="segmentation Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.12423"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.12423"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs</strong><br><em>Penghao Wu, Saining Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/high resolution-blue" alt="high resolution Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.14135"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/penghao-wu/vstar"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.14135"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Cycle-Consistency Learning for Captioning and Grounding</strong><br><em>Ning Wang, Jiajun Deng, Mingbo Jia</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/captioning-blue" alt="captioning Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.15162"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</strong><br><em>Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.17172"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/allenai/unified-io-2"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.17172"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model</strong><br><em>Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/segmentation-blue" alt="segmentation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.17240"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.17240"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models</strong><br><em>Xin He, Longhui Wei, Lingxi Xie, Qi Tian</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.03105"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2401.03105"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Scalable Pre-training of Large Autoregressive Image Models</strong><br><em>Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua M Susskind, Armand Joulin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.08541"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/apple/ml-aim"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2401.08541"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Improving fine-grained understanding in image-text pre-training</strong><br><em>Ioana Bica, Anastasija Iliƒá, Matthias Bauer, Goker Erdogan, Matko Bo≈°njak, Christos Kaplanis, Alexey A. Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, Jovana Mitroviƒá</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.09865"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2401.09865"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities</strong><br><em>Boyuan Chen, Zhuo Xu, Sean Kirmani, Danny Driess, Pete Florence, Brian Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/robotic-blue" alt="robotic Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.12168"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2401.12168"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CCA: Collaborative Competitive Agents for Image Editing</strong><br><em>Tiankai Hang, Shuyang Gu, Dong Chen, Xin Geng, Baining Guo</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.13011"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/tiankaihang/cca"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study</strong><br><em>Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/object detect-blue" alt="object detect Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.17981"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2401.17981"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Dense Reward for Free in Reinforcement Learning from Human Feedback</strong><br><em>Alex J. Chan, Hao Sun, Samuel Holt, Mihaela van der Schaar</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/attention-blue" alt="attention Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.00782"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/xanderjc/attention-based-credit"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.00782"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models</strong><br><em>Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike Zheng Shou</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.01345"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hanmenghan/skip-n"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations</strong><br><em>Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang</em></td>
<td style="width: 20%;"></td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.04236"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/thudm/cogcom"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.04236"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling</strong><br><em>Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, Li Erran Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.06118"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/amazon-science/vigor"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.06118"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>V-STaR: Training Verifiers for Self-Taught Reasoners</strong><br><em>Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.06457"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2402.06457"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Self-Correcting Self-Consuming Loops for Generative Model Training</strong><br><em>Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin Luo, Yonglong Tian, Chen Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.07087"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/nate-gillman/self-correcting-self-consuming"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.07087"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Chain-of-Thought Reasoning Without Prompting</strong><br><em>Xuezhi Wang, Denny Zhou</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.10200"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2402.10200"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models</strong><br><em>Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/plug and play-blue" alt="plug and play Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/logic-blue" alt="logic Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.11622"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hyperwjf/logiccheckgpt"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.11622"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Soft Self-Consistency Improves Language Model Agents</strong><br><em>Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/majority voting-blue" alt="majority voting Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/self consistency-blue" alt="self consistency Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.13212"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hannight/soft_self_consistency"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.13212"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</strong><br><em>Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.13254"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hansolo9682/countercurate"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.13254"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Calibrating Large Language Models with Sample Consistency</strong><br><em>Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Chris Callison-Burch</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/calibration-blue" alt="calibration Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.13904"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</strong><br><em>Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.13950"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2402.13950"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective</strong><br><em>Zihao Yue, Liang Zhang, Qin Jin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.14545"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/yuezih/less-is-more"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.14545"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual Hallucinations of Multi-modal Large Language Models</strong><br><em>Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.14683"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/wenhuang2000/vhtest"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models</strong><br><em>Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, Jiaqi Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.14767"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/DualFocus"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.14767"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images</strong><br><em>Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.14899"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/aipenguin/stopreasoning"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.14899"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding</strong><br><em>Ailin Deng, Zhirui Chen, Bryan Hooi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/training free-blue" alt="training free Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.15300"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/d-ailin/clip-guided-decoding"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</strong><br><em>Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.15527"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/pkunlp-icler/pca-eval"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.15527"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Navigating Hallucinations for Reasoning of Unintentional Activities</strong><br><em>Shresth Grover, Vibhav Vineet, Yogesh S Rawat</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.19405"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning</strong><br><em>Kate Sanders, Nathaniel Weir, Benjamin Van Durme</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.19467"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception</strong><br><em>Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/referring expression-blue" alt="referring expression Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.02969"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/jwh97nn/anyref"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.02969"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM</strong><br><em>Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, Hao Tang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/diffusion-blue" alt="diffusion Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/efficient-blue" alt="efficient Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.07487"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/steve-zeyu-zhang/MotionMamba"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.07487"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring</strong><br><em>Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/high resolution-blue" alt="high resolution Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.09333"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/jefferyzhan/griffon"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.09333"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>GiT: Towards Generalist Vision Transformer through Universal Language Interface</strong><br><em>Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.09394"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/haiyang-w/git"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.09394"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</strong><br><em>Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.09629"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ezelikman/quiet-star"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.09629"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Generative Region-Language Pretraining for Open-Ended Object Detection</strong><br><em>Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, Jianfei Cai</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/object detect-blue" alt="object detect Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/region-blue" alt="region Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.10191"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/foundationvision/generateu"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning</strong><br><em>Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiquing Wang, Pari Delir Haghighi, Hamid Rezatofighi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.12884"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ControlNet/HYDRA"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.12884"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models</strong><br><em>Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, Jiwen Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/region-blue" alt="region Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.12966"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/dongyh20/chain-of-spot"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding</strong><br><em>Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, Fahad Khan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> <img src="https://img.shields.io/badge/self refine-blue" alt="self refine Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.14743"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ahmad-573/vurf"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.14743"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>PropTest: Automatic Property Testing for Improved Visual Programming</strong><br><em>Jaywon Koo, Ziyan Yang, Paola Cascante-Bonilla, Baishakhi Ray, Vicente Ordonez</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/referring expression-blue" alt="referring expression Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.16921"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models</strong><br><em>Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.16999"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/deepcs233/visual-cot"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.16999"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models</strong><br><em>Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, Yin Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/plug and play-blue" alt="plug and play Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/high resolution-blue" alt="high resolution Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.19322"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2403.19322"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance</strong><br><em>Giung Nam, Byeongho Heo, Juho Lee</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/theory-blue" alt="theory Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/distribution shift-blue" alt="distribution shift Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.00860"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/cs-giung/lipsum-ft"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward</strong><br><em>Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, Yiming Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.01258"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/riflezhang/llava-hound-dpo"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2404.01258"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VLRM: Vision-Language Models act as Reward Models for Image Captioning</strong><br><em>Dzabraev Maksim, Kunitsyn Alexander, Ivanyuta Andrey</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/captioning-blue" alt="captioning Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.01911"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2404.01911"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering</strong><br><em>Lili Liang, Guanglu Sun, Jin Qiu, Lizhong Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/program-blue" alt="program Badge"> <img src="https://img.shields.io/badge/symbolic-blue" alt="symbolic Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.04007"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?</strong><br><em>Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.06510"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering</strong><br><em>Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/memory-blue" alt="memory Badge"> <img src="https://img.shields.io/badge/few shot-blue" alt="few shot Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.06511"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2404.06511"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs</strong><br><em>Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S. Ryoo, Tsung-Yu Lin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/data generation-blue" alt="data generation Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.07449"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Revisiting Feature Prediction for Learning Visual Representations from Video</strong><br><em>Adrien Bardes, Adrien Bardes, Adrien Bardes, Adrien Bardes, Adrien Bardes</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.08471"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/facebookresearch/jepa"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2404.08471"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding</strong><br><em>Bozhi Luan, Hao Feng, Hong Chen, Yonghui Wang, Wengang Zhou, Houqiang Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/plug and play-blue" alt="plug and play Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.09797"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/bzluan/textcot"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2404.09797"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function</strong><br><em>Rafael Rafailov, Joey Hejna, Ryan Park, Chelsea Finn</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.12358"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2404.12358"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning</strong><br><em>Mingjie Ma, Zhihuan Yu, Yichao Ma, Guohui Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.13847"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following</strong><br><em>Suyeon Shin, Sujin Jeon, Junghyun Kim, Gi-Cheon Kang, Byoung-Tak Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/embodied-blue" alt="embodied Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.15190"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Step Differences in Instructional Video</strong><br><em>Tushar Nagarajan, Lorenzo Torresani</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/o1-blue" alt="o1 Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.16222"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/facebookresearch/stepdiff"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2404.16222"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Exposing Text-Image Inconsistency Using Diffusion Models</strong><br><em>Mingzhen Huang, Shan Jia, Zhou Zhou, Yan Ju, Jialing Cai, Siwei Lyu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/diffusion-blue" alt="diffusion Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.18033"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/Mingzhen-Huang/D-TIIL"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2404.18033"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning</strong><br><em>Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.00451"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/YuxiXie/MCTS-DPO"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2405.00451"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning</strong><br><em>Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, Ziwei Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.03272"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models</strong><br><em>Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.13872"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Agent Planning with World Knowledge Model</strong><br><em>Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.14205"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/zjunlp/wkm"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2405.14205"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>DynRefer: Delving into Region-level Multi-modality Tasks via Dynamic Resolution</strong><br><em>Yuzhong Zhao, Feng Liu, Yue Liu, Mingxiang Liao, Chen Gong, Qixiang Ye, Fang Wan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.16071"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/callsys/dynrefer"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2405.16071"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos</strong><br><em>Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, Mohit Bansal</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/captioning-blue" alt="captioning Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.19209"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/Ziyang412/VideoTree"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Distributed Ranges: A Model for Distributed Data Structures, Algorithms, and Views</strong><br><em>Benjamin Brock, Robert Cohn, Suyash Bakshi, Tuomas Karna, Jeongnim Kim</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.00158"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning</strong><br><em>Yuwei Fu, Haichao Zhang, Di Wu, Wei Xu, Benoit Boulet</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.00645"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/fuyw/furl"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Cycles of Thought: Measuring LLM Confidence through Stable Explanations</strong><br><em>Evan Becker, Stefano Soatto</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.03441"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Improve Mathematical Reasoning in Language Models by Automated Process Supervision</strong><br><em>Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/self consistency-blue" alt="self consistency Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.06592"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2406.06592"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</strong><br><em>Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.07394"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/trotsky1997/mathblackbox"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.07394"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</strong><br><em>Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.09279"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hamishivi/easylm"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.09279"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living</strong><br><em>Rajatsubhra Chakraborty, Arkaprava Sinha, Dominick Reilly, Manish Kumar, Pu Wang, Francois Bremond, Srijan Das</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.09390"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies</strong><br><em>Hung-Ting Su, Chun-Tong Chao, Ya-Ching Hsu, Xudong Lin, Yulei Niu, Hung-Yi Lee, Winston H. Hsu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.10923"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning</strong><br><em>Zhihan Zhang, Tao Ge, Zhenwen Liang, Wenhao Yu, Dian Yu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.12050"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ytyz1307zzh/RefAug"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.12050"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer</strong><br><em>Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, Kyusong Lee</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.16620"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/om-ai-lab/OmAgent"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.16620"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis</strong><br><em>Chuanqi Cheng, Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.19934"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/steven-ccq/visualreasoner"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.19934"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video Question-Answering</strong><br><em>Zhaohe Liao, Jiangtong Li, Li Niu, Liqing Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.03008"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge</strong><br><em>Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, Lu Yuan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/segmentation-blue" alt="segmentation Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> <img src="https://img.shields.io/badge/retrieval-blue" alt="retrieval Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.04681"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision</strong><br><em>Orr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, Serena Yeung-Levy</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/self training-blue" alt="self training Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.06189"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/orrzohar/Video-STaR"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2407.06189"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models</strong><br><em>Jinrui Zhang, Teng Wang, Haigang Zhang, Ping Lu, Feng Zheng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.11422"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>$VILA^2$: VILA Augmented VILA</strong><br><em>Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, Hongxu Yin</em></td>
<td style="width: 20%;"></td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.17453"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2407.17453"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</strong><br><em>Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.21787"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/scalingintelligence/large_language_monkeys"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2407.21787"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models</strong><br><em>Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, Peilin Zhao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.02032"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/huofushuo/SID"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Mini-Monkey: Alleviate the Sawtooth Effect by Multi-Scale Adaptive Cropping</strong><br><em>Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, Xiang Bai</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/efficient-blue" alt="efficient Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.02034"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/yuliang-liu/monkey"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.02034"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</strong><br><em>Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.03314"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2408.03314"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>How Well Can Vision Language Models See Image Details?</strong><br><em>Chenhui Gou, Abdulwahab Felemban, Faizan Farooq Khan, Deyao Zhu, Jianfei Cai, Hamid Rezatofighi, Mohamed Elhoseiny</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.03940"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Instruction Tuning-free Visual Token Complement for Multimodal LLMs</strong><br><em>Dongsheng Wang, Jiequan Cui, Miaoge Li, Wang Lin, Bo Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.05019"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers</strong><br><em>Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.06195"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/zhentingqi/rstar"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.06195"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents</strong><br><em>Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.07199"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2408.07199"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs</strong><br><em>Yassine Ouali, Adrian Bulat, Brais Martinez, Georgios Tzimiropoulos</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.10433"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2408.10433"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs</strong><br><em>Yuanyang Yin, Yaqi Zhao, Yajie Zhang, Ke Lin, Jiahao Wang, Xin Tao, Pengfei Wan, Di Zhang, Baoqun Yin, Wentao Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.11813"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2408.11813"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Making Large Language Models Better Planners with Reasoning-Decision Alignment</strong><br><em>Zhijian Huang, Tao Tang, Shaoxiang Chen, Sihao Lin, Zequn Jie, Lin Ma, Guangrun Wang, Xiaodan Liang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.13890"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2408.13890"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos</strong><br><em>Qirui Chen, Shangzhe Di, Weidi Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.14469"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2408.14469"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning</strong><br><em>Xiaoye Qu, Jiashuo Sun, Wei Wei, Yu Cheng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/training free-blue" alt="training free Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.17150"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/gasolsun36/mvp"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.17150"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Self-Harmonized Chain of Thought</strong><br><em>Ziqi Jin, Wei Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.04057"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/Xalp/ECHO"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2409.04057"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>What Makes a Maze Look Like a Maze?</strong><br><em>Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.08202"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2409.08202"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Training Language Models to Self-Correct via Reinforcement Learning</strong><br><em>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, JD Co-Reyes</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/self correction-blue" alt="self correction Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.12917"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2409.12917"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual Question Decomposition on Multimodal Large Language Models</strong><br><em>Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.19339"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2409.19339"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Interpretable Contrastive Monte Carlo Tree Search Reasoning</strong><br><em>Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.01707"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/zitian-gao/sc-mcts"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.01707"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLaVA-Critic: Learning to Evaluate Multimodal Models</strong><br><em>Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.02712"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.02712"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering</strong><br><em>Jin Chen, Kaijing Ma, Haojian Huang, Jiayu Shen, Han Fang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/self training-blue" alt="self training Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.02768"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning</strong><br><em>Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/self refine-blue" alt="self refine Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.02884"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.02884"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning</strong><br><em>Minheng Ni, Yutao Fan, Lei Zhang, Wangmeng Zuo</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.03321"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models</strong><br><em>Xin Zou, Yizhou Wang, Yibo Yan, Sirui Huang, Kening Zheng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/memory-blue" alt="memory Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.03577"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/1zhou-Wang/MemVR"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>TLDR: Token-Level Detective Reward Model for Large Vision Language Models</strong><br><em>Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.04734"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.04734"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning</strong><br><em>Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/distillation-blue" alt="distillation Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.06508"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.06508"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision</strong><br><em>Shengcao Cao, Liang-Yan Gui, Yu-Xiong Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/segmentation-blue" alt="segmentation Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/diffusion-blue" alt="diffusion Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/attention-blue" alt="attention Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.08209"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment</strong><br><em>Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.09421"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.09421"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Reconstructive Visual Instruction Tuning</strong><br><em>Haochen Wang, Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.09575"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.09575"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>AFlow: Automating Agentic Workflow Generation</strong><br><em>Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.10762"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/geekan/metagpt"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.10762"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models</strong><br><em>Mazda Moayeri, Vidhisha Balachandran, Varun Chadrasekaran, Safoora Yousefi, Thomas Fel</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.13826"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Improve Vision Language Model Chain-of-thought Reasoning</strong><br><em>Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/distillation-blue" alt="distillation Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.16198"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/riflezhang/llava-reasoner-dpo"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.16198"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Aligning Large Language Models via Self-Steering Optimization</strong><br><em>Hao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.17131"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/icip-cas/sso"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.17131"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models</strong><br><em>Linger Deng, Yuliang Liu, Bohan Li, Dongliang Luo, Liang Wu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.17885"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/dle666/r-cot"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.17885"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement</strong><br><em>Antonis Antoniades, Albert ¬®Orwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.20285"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/aorwall/moatless-tools"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.20285"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LongReward: Improving Long-context Large Language Models with AI Feedback</strong><br><em>Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/logic-blue" alt="logic Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.21252"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/THUDM/LongReward"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.21252"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Effective Guidance for Model Attention with Simple Yes-no Annotations</strong><br><em>Seongmin Lee, Ali Payani, Duen Horng (Polo) Chau</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/attention-blue" alt="attention Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.22312"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/poloclub/crayon"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Natural Language Inference Improves Compositionality in Vision-Language Models</strong><br><em>Paola Cascante-Bonilla, Yu Hou, Yang Trista Cao, Hal Daum¬¥e III, Rachel Rudinger</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.22315"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Vision-Language Models Can Self-Improve Reasoning via Reflection</strong><br><em>Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/self training-blue" alt="self training Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/self refine-blue" alt="self refine Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.00855"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2411.00855"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>The Surprising Effectiveness of Test-Time Training for Abstract Reasoning</strong><br><em>Ekin Aky√ºrek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.07279"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ekinakyurek/marc"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.07279"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLaVA-o1: Let Vision Language Models Reason Step-by-Step</strong><br><em>Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.10440"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/PKU-YuanGroup/LLaVA-CoT"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.10440"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization</strong><br><em>Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/distribution shift-blue" alt="distribution shift Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.10442"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2411.10442"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Enhancing LLM Reasoning with Reward-guided Tree Search</strong><br><em>Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.11694"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2411.11694"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination</strong><br><em>Haojie Zheng, Tianyang Xu, Hanchi Sun, Shu Pu, Ruoxi Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.12591"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/terry-xu-666/visual_inference_chain"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.12591"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Natural Language Reinforcement Learning</strong><br><em>Xidong Feng, Ziyu Wan, Haotian Fu, Bo Liu, Mengyue Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/value function-blue" alt="value function Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.14251"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/waterhorse1/natural-language-rl"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.14251"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</strong><br><em>Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.14405"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/aidc-ai/marco-o1"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.14405"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability</strong><br><em>Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.19943"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/chenzhiling9954/critical-tokens-matter"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.19943"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Unlocking Video-LLM via Agent-of-Thoughts Distillation</strong><br><em>Yudi Shi, Shangzhe Di, Qirui Chen, Weidi Xie, null</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.01694"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Progress-Aware Video Frame Captioning</strong><br><em>Zihui Xue, Joungbin An, Xitong Yang, Kristen Grauman</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.02071"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Perception Tokens Enhance Visual Reasoning in Multimodal Language Models</strong><br><em>Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/object detect-blue" alt="object detect Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.03548"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.03548"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension</strong><br><em>Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.03704"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/si0wang/visvm"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.03704"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation</strong><br><em>Yongxin Wang, Meng Cao, Haokun Lin, Mingfei Han, Liang Ma</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.04903"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CompCap: Improving Multimodal Large Language Models with Composite Captions</strong><br><em>Xiaohui Chen, Satya Narayan Shukla, Mahmoud Azab, Aashu Singh, Qifan Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.05243"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.05243"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action</strong><br><em>Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.05479"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/salesforceairesearch/taco"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.05479"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models</strong><br><em>Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/data generation-blue" alt="data generation Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.07012"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/jieyuz2/provision"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.07012"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MM-PoE: Multiple Choice Reasoning via. Process of Elimination using Multi-Modal Models</strong><br><em>Sayak Chakrabarty, Souradip Pal</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/few shot-blue" alt="few shot Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/zero shot-blue" alt="zero shot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.07148"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/souradipp76/mm-poe"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can We Generate Visual Programs Without Prompting LLMs?</strong><br><em>Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.08564"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Multimodal Latent Language Modeling with Next-Token Diffusion</strong><br><em>Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/diffusion-blue" alt="diffusion Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.08635"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/microsoft/unilm/tree/master/LatentLM"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.08635"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ViUniT: Visual Unit Tests for More Robust Visual Programming</strong><br><em>Artemis Panagopoulou, Honglu Zhou, Silvio Savarese, Caiming Xiong, Chris Callison-Burch</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.08859"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems</strong><br><em>Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/o1-blue" alt="o1 Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.09413"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.09413"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>TimeRefine: Temporal Grounding with Time Refining Video LLM</strong><br><em>Xizi Wang, Feng Cheng, Ziyang Wang, Huiyu Wang, Md Mohaiminul Islam</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.09601"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/SJTUwxz/TimeRefine"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VCA: Video Curious Agent for Long Video Understanding</strong><br><em>Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, Chuang Gan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/efficient-blue" alt="efficient Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.10471"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Combating Multimodal LLM Hallucination via Bottom-up Holistic Reasoning</strong><br><em>Shengqiong Wu, Hao Fei, Liangming Pan, William Yang Wang, Shuicheng Yan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.11124"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning</strong><br><em>Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Min Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.13540"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/aaandy-zhu/vgcure"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective</strong><br><em>Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.14135"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.14135"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</strong><br><em>Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.15084"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.15084"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Diving into Self-Evolving Training for Multimodal Reasoning</strong><br><em>Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.17451"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.17451"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</strong><br><em>Jiaan Wang, Fandong Meng, Yunlong Liang, Jie Zhou</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.17498"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/krystalan/drt-o1"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.17498"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MMFactory: A Universal Solution Search Engine for Vision-Language Tasks</strong><br><em>Wan-Cyuan Fan, Tanzila Rahman, Leonid Sigal</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.18072"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.18072"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search</strong><br><em>Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.18319"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hjyao00/mulberry"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.18319"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Token-Budget-Aware LLM Reasoning</strong><br><em>Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.18547"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/geniushtx/tale"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.18547"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Efficiently Serving LLM Reasoning Programs with Certaindex</strong><br><em>Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.20993"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.20993"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</strong><br><em>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/self training-blue" alt="self training Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.21187"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.21187"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning</strong><br><em>Pu Yang, Bin Dong</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/captioning-blue" alt="captioning Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.01834"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?</strong><br><em>Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.02669"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/princeton-pli/vlm_s2h"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.02669"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion</strong><br><em>Zhaoyi Yan, Zhijie Sang, Yiming Zhang, Yuhao Fu, Baoyi He</em></td>
<td style="width: 20%;"></td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.02795"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild</strong><br><em>Wanpeng Hu, Haodi Liu, Lin Chen, Feng Zhou, Changming Xiao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.02964"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/aibee00/socraticquestioning"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.02964"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition</strong><br><em>Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.03230"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning</strong><br><em>Andrew Li, Rahul Thapa, Rahul Chalamala, Qingyang Wu, Kezhen Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.03675"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/togethercomputer/smir"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.03675"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</strong><br><em>Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.04519"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.04519"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought</strong><br><em>Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> <img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.04682"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.04682"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning</strong><br><em>Huabin Liu, Filip Ilievski, Cees G. M. Snoek</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.05069"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Enabling Scalable Oversight via Self-Evolving Critic</strong><br><em>Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.05727"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.05727"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs</strong><br><em>Shan Zhang, Aotian Chen, Yanpeng Sun, Jindong Gu, Yi-Yu Zheng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/grounding-blue" alt="grounding Badge"> <img src="https://img.shields.io/badge/contrastive-blue" alt="contrastive Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.06430"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ai4math-shanzhang/sve-math"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.06430"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning</strong><br><em>Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> <img src="https://img.shields.io/badge/medical-blue" alt="medical Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.06458"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/spiral-med/ophiuchus"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.06458"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</strong><br><em>Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.07542"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.07542"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation</strong><br><em>Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fern√°ndez Fisac</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.08617"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.08617"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking</strong><br><em>Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/retrieval-blue" alt="retrieval Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.09751"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/zjunlp/omnithink"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.09751"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Evolving Deeper LLM Thinking</strong><br><em>Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.09891"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.09891"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback</strong><br><em>Yen-Ting Lin, Di Jin, Tengyu Xu, Tianhao Wu, Sainbayar Sukhbaatar</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> <img src="https://img.shields.io/badge/self consistency-blue" alt="self consistency Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.10799"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.10799"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement</strong><br><em>Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> <img src="https://img.shields.io/badge/data generation-blue" alt="data generation Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.12273"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/internlm/condor"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.12273"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</strong><br><em>Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.12368"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/internlm/internlm-xcomposer"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.12368"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ReasVQA: Advancing VideoQA with Imperfect Reasoning Process</strong><br><em>Jianxin Liang, Xiaojun Meng, Huishuai Zhang, Yueqian Wang, Jiansheng Wei</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.13536"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</strong><br><em>Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.13926"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ziyuguo99/image-generation-cot"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.13926"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</strong><br><em>Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.18585"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.18585"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception</strong><br><em>Joshua R. Waite, Md. Zahid Hasan, Qisai Liu, Zhanhong Jiang, Chinmay Hegde</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.18880"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>s1: Simple test-time scaling</strong><br><em>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.19393"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/simplescaling/s1"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.19393"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Masked Generative Nested Transformers with Decode Time Scaling</strong><br><em>Sahil Goyal, Debapriya Tula, Gagan Jain, Pradeep Shenoy, Prateek Jain</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2502.00382"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2502.00382"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework</strong><br><em>Chunbai Zhang, Chao Wang, Yang Zhou, Yan Peng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reflect-blue" alt="reflect Badge"> <img src="https://img.shields.io/badge/distillation-blue" alt="distillation Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2502.00711"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2502.00711"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking</strong><br><em>Jinyang Wu, Mingkuan Feng, Shuai Zhang, Ruihan Jin, Feihu Che</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/o1-blue" alt="o1 Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2502.02339"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2502.02339"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LIMO: Less is More for Reasoning</strong><br><em>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2502.03387"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/gair-nlp/limo"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2502.03387"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</strong><br><em>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.01307"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/kanishkg/cognitive-behaviors"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.01307"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual-RFT: Visual Reinforcement Fine-Tuning</strong><br><em>Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.01785"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/liuziyu77/visual-rft"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.01785"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching</strong><br><em>Simon A. Aytes, Jinheon Baek, Sung Ju Hwang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.05179"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/simonaytes/sot"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.05179"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning</strong><br><em>Unknown</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.05379"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2503.05379"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?</strong><br><em>Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.06252"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/quinn777/atomthink"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.06252"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning</strong><br><em>Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.07365"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/modalminds/mm-eureka"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.07365"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL</strong><br><em>Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.07536"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2503.07536"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can Memory-Augmented Language Models Generalize on Reasoning-in-a-Haystack Tasks?</strong><br><em>Payel Das, Ching-Yun Ko, Sihui Dai, Georgios Kollias, Subhajit Chaudhury</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/memory-blue" alt="memory Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.07903"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of Role-Playing Language Agents</strong><br><em>Rui Xu, MingYu Wang, XinTao Wang, Dakuan Lu, Xiaoyu Tan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/memory-blue" alt="memory Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.08193"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training</strong><br><em>Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/embodied-blue" alt="embodied Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.08525"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2503.08525"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs</strong><br><em>Wanyong Feng, Peter Tran, Stephen Sireci, Andrew Lan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/theory-blue" alt="theory Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.08551"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Rule-Guided Reinforcement Learning Policy Evaluation and Improvement</strong><br><em>Martin Tappler, Ignacio D. Lopez-Miguel, Sebastian Tschiatschek, Ezio Bartocci</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.09270"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning</strong><br><em>Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.09501"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2503.09501"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</strong><br><em>Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/retrieval-blue" alt="retrieval Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.09516"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/petergriffinjin/search-r1"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.09516"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model</strong><br><em>Bowen Zhang, Pengcheng Luo, Pengcheng Luo</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.10009"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/bwz96sco/or_llm_agent"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes through Tree-of-Error</strong><br><em>Shu-Xun Yang, Cunxiang Wang, Yidong Wang, Xiaotao Gu, Minlie Huang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.10105"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/shu-xun/stepmathagent"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
</table>


<p align="right"><a href="#-table-of-contents">Back to Top</a></p>

### Theory <a id='theory'></a>

<table style="width: 100%;">
<tr>
<td><strong>Date</strong></td>
<td><strong>Title & Authors</strong></td>
<td><strong>Tags</strong></td>
<td><strong>MLLM Type</strong></td>
<td><strong>Links</strong></td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning</strong><br><em>Zhiting Hu, Tianmin Shu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.05230"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.05230"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Self-Correcting Self-Consuming Loops for Generative Model Training</strong><br><em>Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin Luo, Yonglong Tian, Chen Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.07087"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/nate-gillman/self-correcting-self-consuming"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.07087"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function</strong><br><em>Rafael Rafailov, Joey Hejna, Ryan Park, Chelsea Finn</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.12358"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2404.12358"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Distributed Ranges: A Model for Distributed Data Structures, Algorithms, and Views</strong><br><em>Benjamin Brock, Robert Cohn, Suyash Bakshi, Tuomas Karna, Jeongnim Kim</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.00158"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Large Language Models as Markov Chains</strong><br><em>Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boull¬¥e</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.02724"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.02724"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem</strong><br><em>Declan Campbell, Sunayana Rane, Tyler Giallanza, Nicol√≤ De Sabbata, Kia Ghods</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.00238"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Natural Language Reinforcement Learning</strong><br><em>Xidong Feng, Ziyu Wan, Haotian Fu, Bo Liu, Mengyue Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/value function-blue" alt="value function Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.14251"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/waterhorse1/natural-language-rl"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.14251"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</strong><br><em>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.01307"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/kanishkg/cognitive-behaviors"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.01307"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
</table>


<p align="right"><a href="#-table-of-contents">Back to Top</a></p>

### Application/System <a id='application-system'></a>

<table style="width: 100%;">
<tr>
<td><strong>Date</strong></td>
<td><strong>Title & Authors</strong></td>
<td><strong>Tags</strong></td>
<td><strong>MLLM Type</strong></td>
<td><strong>Links</strong></td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</strong><br><em>Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2309.15112"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/internlm/internlm-xcomposer"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2309.15112"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</strong><br><em>Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, Jianwei Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.02949"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ux-decoder/llava-grounding"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.02949"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models</strong><br><em>Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/retrieval-blue" alt="retrieval Badge"> <img src="https://img.shields.io/badge/distillation-blue" alt="distillation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.03052"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.03052"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Silkie: Preference Distillation for Large Visual Language Models</strong><br><em>Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.10665"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.10665"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Jack of All Tasks, Master of Many_ Designing General-purpose Coarse-to-Fine Vision-Language Model</strong><br><em>Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/segmentation-blue" alt="segmentation Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.12423"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.12423"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</strong><br><em>Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/auto regressive-blue" alt="auto regressive Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.17172"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/allenai/unified-io-2"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.17172"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities</strong><br><em>Boyuan Chen, Zhuo Xu, Sean Kirmani, Danny Driess, Pete Florence, Brian Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/robotic-blue" alt="robotic Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.12168"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2401.12168"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study</strong><br><em>Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/object detect-blue" alt="object detect Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.17981"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2401.17981"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring</strong><br><em>Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/high resolution-blue" alt="high resolution Badge"> <img src="https://img.shields.io/badge/vision language-blue" alt="vision language Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.09333"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/jefferyzhan/griffon"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.09333"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Step Differences in Instructional Video</strong><br><em>Tushar Nagarajan, Lorenzo Torresani</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/o1-blue" alt="o1 Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.16222"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/facebookresearch/stepdiff"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2404.16222"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models</strong><br><em>Pengxiang Li, Zhi Gao, Bofei Zhang, Tao Yuan, Yuwei Wu, Mehrtash Harandi, Yunde Jia, Song-Chun Zhu, Qing Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.11522"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2407.11522"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Mini-Monkey: Alleviate the Sawtooth Effect by Multi-Scale Adaptive Cropping</strong><br><em>Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, Xiang Bai</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/efficient-blue" alt="efficient Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.02034"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/yuliang-liu/monkey"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.02034"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>How Well Can Vision Language Models See Image Details?</strong><br><em>Chenhui Gou, Abdulwahab Felemban, Faizan Farooq Khan, Deyao Zhu, Jianfei Cai, Hamid Rezatofighi, Mohamed Elhoseiny</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.03940"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models</strong><br><em>Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Dacheng Tao</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/high resolution-blue" alt="high resolution Badge"> <img src="https://img.shields.io/badge/training free-blue" alt="training free Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.15556"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/DreamMr/HR-Bench"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.15556"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension</strong><br><em>Junzhuo Liu, Xuzheng Yang, Weiwei Li, Peng Wang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/referring expression-blue" alt="referring expression Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.14750"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/liujunzhuo/FineCops-Ref"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Visual Question Decomposition on Multimodal Large Language Models</strong><br><em>Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.19339"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2409.19339"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLaVA-Critic: Learning to Evaluate Multimodal Models</strong><br><em>Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.02712"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2410.02712"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering</strong><br><em>Jin Chen, Kaijing Ma, Haojian Huang, Jiayu Shen, Han Fang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/self training-blue" alt="self training Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.02768"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>The Surprising Effectiveness of Test-Time Training for Abstract Reasoning</strong><br><em>Ekin Aky√ºrek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.07279"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ekinakyurek/marc"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2411.07279"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Perception Tokens Enhance Visual Reasoning in Multimodal Language Models</strong><br><em>Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/object detect-blue" alt="object detect Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.03548"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.03548"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action</strong><br><em>Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.05479"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/salesforceairesearch/taco"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.05479"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ViUniT: Visual Unit Tests for More Robust Visual Programming</strong><br><em>Artemis Panagopoulou, Honglu Zhou, Silvio Savarese, Caiming Xiong, Chris Callison-Burch</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.08859"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>From Simple to Professional: A Combinatorial Controllable Image Captioning Agent</strong><br><em>Xinran Wang, Muxi Diao, Baoteng Li, Haiwen Zhang, Kongming Liang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/captioning-blue" alt="captioning Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.11025"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/xin-ran-w/capagent"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</strong><br><em>Jiaan Wang, Fandong Meng, Yunlong Liang, Jie Zhou</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.17498"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/krystalan/drt-o1"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.17498"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MMFactory: A Universal Solution Search Engine for Vision-Language Tasks</strong><br><em>Wan-Cyuan Fan, Tanzila Rahman, Leonid Sigal</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/program-blue" alt="program Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.18072"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.18072"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Efficiently Serving LLM Reasoning Programs with Certaindex</strong><br><em>Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.20993"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.20993"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition</strong><br><em>Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/video-blue" alt="video Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.03230"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning</strong><br><em>Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> <img src="https://img.shields.io/badge/medical-blue" alt="medical Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.06458"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/spiral-med/ophiuchus"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.06458"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</strong><br><em>Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.12368"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/internlm/internlm-xcomposer"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.12368"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception</strong><br><em>Joshua R. Waite, Md. Zahid Hasan, Qisai Liu, Zhanhong Jiang, Chinmay Hegde</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/synthetic data-blue" alt="synthetic data Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.18880"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs</strong><br><em>Wanyong Feng, Peter Tran, Stephen Sireci, Andrew Lan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/theory-blue" alt="theory Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.08551"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
</table>


<p align="right"><a href="#-table-of-contents">Back to Top</a></p>

### Survey <a id='survey'></a>

<table style="width: 100%;">
<tr>
<td><strong>Date</strong></td>
<td><strong>Title & Authors</strong></td>
<td><strong>Tags</strong></td>
<td><strong>MLLM Type</strong></td>
<td><strong>Links</strong></td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Language-conditioned Learning for Robotic Manipulation: A Survey</strong><br><em>Hongkuan Zhou, Xiangtong Yao, Yuan Meng, Siming Sun, Zhenshan Bing, Kai Huang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/robotic-blue" alt="robotic Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.10807"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hk-zh/language-conditioned-robot-manipulation-models"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2312.10807"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>A Survey of Reasoning with Foundation Models</strong><br><em>Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, Zhenguo Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.11562"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/reasoning-survey/awesome-reasoning-foundation-models"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models</strong><br><em>Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.01230"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey</strong><br><em>Philipp Mondorf, Barbara Plank</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.01869"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Reasoning with Large Language Models, a Survey</strong><br><em>Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas B¬® ack</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/few shot-blue" alt="few shot Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2407.11511"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2407.11511"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>A Survey on Benchmarks of Multimodal Large Language Models</strong><br><em>Jian Li, Weiheng Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.08632"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/swordlidev/evaluation-multimodal-llms-survey"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2408.08632"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Reinforcement Learning Enhanced LLMs: A Survey</strong><br><em>Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.10400"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/shuhewang1998/reinforcement-learning-enhanced-llms-a-survey"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.10400"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Reasoning based on symbolic and parametric knowledge bases: a survey</strong><br><em>Mayi Xu, Yunfeng Ning, Yongqi Li, Jianhao Chen, Jintao Wen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/symbolic-blue" alt="symbolic Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.01030"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models</strong><br><em>Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> <img src="https://img.shields.io/badge/o1-blue" alt="o1 Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.09686"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.09686"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models</strong><br><em>Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.09567"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2503.09567"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
</table>


<p align="right"><a href="#-table-of-contents">Back to Top</a></p>

### Analysis/Evaluation <a id='analysis-evaluation'></a>

<table style="width: 100%;">
<tr>
<td><strong>Date</strong></td>
<td><strong>Title & Authors</strong></td>
<td><strong>Tags</strong></td>
<td><strong>MLLM Type</strong></td>
<td><strong>Links</strong></td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</strong><br><em>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/supervised fine tuning-blue" alt="supervised fine tuning Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2308.01825"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/ofa-sys/gsm8k-screl"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2308.01825"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>What's "up" with vision-language models? Investigating their struggle with spatial reasoning</strong><br><em>Amita Kamath, Jack Hessel, Kai-Wei Chang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2310.19785"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/amitakamath/whatsup_vlms"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2310.19785"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning</strong><br><em>Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, Cihang Xie</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/attention-blue" alt="attention Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2312.11420"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2312.11420"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models</strong><br><em>Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, Nanyun Peng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2401.13311"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/rohan598/contextual"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2401.13311"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</strong><br><em>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.04788"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/dongping-chen/mllm-as-a-judge"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.04788"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Exploring Perceptual Limitation of Multimodal Large Language Models</strong><br><em>Jiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, Maosong Sun</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.07384"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/saccharomycetes/mllm-perceptual-limitation"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.07384"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Calibrating Large Language Models with Sample Consistency</strong><br><em>Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Chris Callison-Burch</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/calibration-blue" alt="calibration Badge"> <img src="https://img.shields.io/badge/dataset-blue" alt="dataset Badge"> <img src="https://img.shields.io/badge/instruction tun-blue" alt="instruction tun Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.13904"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</strong><br><em>Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.13950"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2402.13950"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images</strong><br><em>Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.14899"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/aipenguin/stopreasoning"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2402.14899"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Do Large Language Models Latently Perform Multi-Hop Reasoning?</strong><br><em>Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/scaling-blue" alt="scaling Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2402.16837"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2402.16837"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks</strong><br><em>Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/tool-blue" alt="tool Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2403.11085"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/RAIVNLab/mms"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2403.11085"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>BLINK: Multimodal Large Language Models Can See but Not Perceive</strong><br><em>Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2404.12390"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2404.12390"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</strong><br><em>Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, Wanxiang Che</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2405.16473"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/LightChen233/M3CoT"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2405.16473"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs</strong><br><em>Irene Huang, Wei Lin, M. Jehanzeb Mirza, Jacob A. Hansen, Sivan Doveh, Victor Ion Butoi, Roei Herzig, Hilde Kuhene, Trevor Darrel, Chuang Gan, Aude Oliva, Rogerio Feris, Leonid Karlinsky</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/data generation-blue" alt="data generation Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.08164"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/jmiemirza/conme"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.08164"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</strong><br><em>Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/policy-blue" alt="policy Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2406.09279"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hamishivi/easylm"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2406.09279"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</strong><br><em>Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2408.03314"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2408.03314"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability</strong><br><em>Kevin Wang, Junbo Li, Neel P. Bhatt, Yihan Xi, Qiang Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> <img src="https://img.shields.io/badge/planning-blue" alt="planning Badge"> <img src="https://img.shields.io/badge/memory-blue" alt="memory Badge"> <img src="https://img.shields.io/badge/self evaluation-blue" alt="self evaluation Badge"> <img src="https://img.shields.io/badge/spatial-blue" alt="spatial Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2409.19924"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/vita-group/o1-planning"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2409.19924"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models</strong><br><em>Ziyue Wang, Chi Chen, Fuwen Luo, Yurui Dong, Yuanchi Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.04659"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/THUNLP-MT/ActiView"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.04659"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>A Comparative Study on Reasoning Patterns of OpenAI's o1 Model</strong><br><em>Siwei Wu, Siwei Wu, Siwei Wu, Zhongyuan Peng, Xinrun Du</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/math-blue" alt="math Badge"> <img src="https://img.shields.io/badge/agent-blue" alt="agent Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/o1-blue" alt="o1 Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.13639"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/open-source-o1/o1_reasoning_patterns_study"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2410.13639"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models</strong><br><em>Mazda Moayeri, Vidhisha Balachandran, Varun Chadrasekaran, Safoora Yousefi, Thomas Fel</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/evaluation-blue" alt="evaluation Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2410.13826"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models</strong><br><em>Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/verification-blue" alt="verification Badge"> <img src="https://img.shields.io/badge/hallucination-blue" alt="hallucination Badge"> <img src="https://img.shields.io/badge/inference time-blue" alt="inference time Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2411.17451"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2411.17451"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning</strong><br><em>Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/self improv-blue" alt="self improv Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/fine grained-blue" alt="fine grained Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.02172"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.02172"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models</strong><br><em>Zihui Cheng, Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/lvlm-blue" alt="lvlm Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.12932"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/czhhzc/CoMT"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.12932"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Are Your LLMs Capable of Stable Reasoning?</strong><br><em>Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/metric-blue" alt="metric Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.13147"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/open-compass/gpassk"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2412.13147"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Diving into Self-Evolving Training for Multimodal Reasoning</strong><br><em>Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/reward model-blue" alt="reward model Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.17451"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.17451"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</strong><br><em>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/self training-blue" alt="self training Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2412.21187"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2412.21187"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark</strong><br><em>Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/test time-blue" alt="test time Badge"> <img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/cot-blue" alt="cot Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.05444"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/hychaochao/EMMA"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2501.05444"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation</strong><br><em>Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fern√°ndez Fisac</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/preference-blue" alt="preference Badge"> <img src="https://img.shields.io/badge/rl-blue" alt="rl Badge"> <img src="https://img.shields.io/badge/dpo-blue" alt="dpo Badge"> <img src="https://img.shields.io/badge/feedback-blue" alt="feedback Badge"> <img src="https://img.shields.io/badge/alignment-blue" alt="alignment Badge"> <img src="https://img.shields.io/badge/optimization-blue" alt="optimization Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.08617"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.08617"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No!</strong><br><em>Mohamed Fazli Imam, Chenyang Lyu</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/mllm-blue" alt="mllm Badge"> <img src="https://img.shields.io/badge/temporal-blue" alt="temporal Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">MLLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.10674"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.10674"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</strong><br><em>Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2501.18585"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://huggingface.co/papers/2501.18585"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>LIMO: Less is More for Reasoning</strong><br><em>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/efficiency-blue" alt="efficiency Badge"> <img src="https://img.shields.io/badge/math-blue" alt="math Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2502.03387"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/gair-nlp/limo"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2502.03387"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
<tr>
<td style="width: 10%;">2025-04-05</td>
<td style="width: 40%;"><strong>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning</strong><br><em>Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang</em></td>
<td style="width: 20%;"><img src="https://img.shields.io/badge/benchmark-blue" alt="benchmark Badge"> <img src="https://img.shields.io/badge/multi-blue" alt="multi Badge"> <img src="https://img.shields.io/badge/search-blue" alt="search Badge"> <img src="https://img.shields.io/badge/qa-blue" alt="qa Badge"> </td>
<td style="width: 15%;">LLM</td>
<td style="width: 15%;"><a href="https://arxiv.org/abs/2503.07459"><img src="https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv" alt="Paper Badge"></a> <a href="https://github.com/gersteinlab/medagents-benchmark"><img src="https://img.shields.io/badge/GitHub-Code-%23181717?logo=github" alt="GitHub Badge"></a> <a href="https://huggingface.co/papers/2503.07459"><img src="https://img.shields.io/badge/HuggingFace-Model-%23FFD43B?logo=huggingface" alt="HuggingFace Badge"></a> </td>
</tr>
</table>


<p align="right"><a href="#-table-of-contents">Back to Top</a></p>

## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jing-bi/awesome-M.LLM-reasoning&type=Date)](https://star-history.com/#jing-bi/awesome-M.LLM-reasoning&Date)

## ‚úèÔ∏è Citation
If you find our survey useful, please cite the following paper:
```bibtex
@misc{bi2025reasoningmatterssurveyadvancements,
      title={Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)}, 
      author={Jing Bi and Susan Liang and Xiaofei Zhou and Pinxin Liu and Junjia Guo and Yunlong Tang and Luchuan Song and Chao Huang and Guangyu Sun and Jinxi He and Jiarui Wu and Shu Yang and Daoan Zhang and Chen Chen and Lianggong Bruce Wen and Zhang Liu and Jiebo Luo and Chenliang Xu},
      year={2025},
      eprint={2504.03151},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.03151}, 
}
```
